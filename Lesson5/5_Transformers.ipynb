{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e06b88",
   "metadata": {},
   "source": [
    "# Attention and Transformers   \n",
    "\n",
    "## Machine Learning 530\n",
    "\n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079cd15f",
   "metadata": {},
   "source": [
    "## Introduction   \n",
    "\n",
    "This lab will give you some hands on experience training an English to Spanish translation model using transformers. This is a simple example of a sequence-to-sequence model used in natural language processing (NLP). \n",
    "\n",
    "Before continuing, you will need to establish a Google Colab account. If you do not have an account you can sign up on [this page](https://colab.research.google.com/signup). A free account will be adequate for this lab. \n",
    "\n",
    "------------------\n",
    "> **Note:** If you do not wish to run the notebook in Colab, you can download the example notebook and run it in another environment. Depending on the environment, doing so may require minor changes in the notebooks. \n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc89dae",
   "metadata": {},
   "source": [
    "## Starting and running the notebook\n",
    "\n",
    "You will now start and execute a Jupyter notebook containing a Keras transformer example. Go to [this page](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/), and click **View in Colab**. Once the notebook launches in Colab read the provided commentary and examine the code for each cell. Then, execute the code all the cells in order. The model training runs for 1302 epochs, which will take some time.       \n",
    "\n",
    "> **Note:** For this assignment, you are required to submit this notebook with the exercises complete along with the executed notebook from Colab. To submit your executed notebook you must use File -> Download -> Download .ipynb. You can then upload your notebook into Canvas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c794a",
   "metadata": {},
   "source": [
    "> **Exercise 5-1:** Examine the code used to instantiate the `TransformerEncoder` class and provide short answers to the following questions: \n",
    "> 1. In terms of the three types of attention models, which one is used as a layer in the encoder and why? **Hint:** Examine the code for the `call` method. \n",
    "> 2. How many heads does the transformer layer in the encoder use?    \n",
    "> 3. If you wanted to increase the model capacity, how would you change this hyperparameter? \n",
    "> 4. What will this change in hyperparameter affect amount of training data required, the computing time required and the chance of over-fitting the model? \n",
    "> 5. How is the dimension of the encoding transformer layer key vector specified, why is the the correct choice, and what is the value? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db58a98",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.      \n",
    "> 2.      \n",
    "> 3.       \n",
    "> 4.        \n",
    "> 5.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a6c40",
   "metadata": {},
   "source": [
    "> **Exercise 5-2:** Examine the code used to instantiate the `PositionalEmbedding` class and provide short answers to the following questions:\n",
    "> 1. Does the encoder layer use positional embedding? **Hint:** To find out, look at the code used and summary produced for training the model.  \n",
    "> 2. Why is positional encoding a correct choice, considering the structure of the queries, for this application? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4796444f",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.      \n",
    "> 2.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24c75f",
   "metadata": {},
   "source": [
    "> **Exercise 5-3:** Examine the code used to instantiate the `TransformerDecoder` class and provide short answers to the following questions:  \n",
    "> 1. In terms of the three types of attention models, which one is used for the `attention_1` layer in the encoder and why? **Hint:** Examine the code for the `call` method.   \n",
    "> 2. Why is the choice of attention model you noted for the answer to the previous question appropriate for this layer? \n",
    "> 3. In terms of the three types of attention models, which one is used for the `attention_2` layer in the encoder and why? **Hint:** Examine the code for the `call` method.  \n",
    "> 4. Why is the choice of attention model you noted for the answer to the previous question appropriate for this layer? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592a643",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.      \n",
    "> 2.      \n",
    "> 3.      \n",
    "> 4.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6c120",
   "metadata": {},
   "source": [
    "#### Copyright 2022, Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
