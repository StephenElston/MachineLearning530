{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks\n",
    "\n",
    "### Machine Learning 410\n",
    "### Steve Elston\n",
    "\n",
    "## 1.0 Introduction to neural networks\n",
    "\n",
    "In this lesson you will explore **recurrent neural networks (RNNs)**. Recurrent neural networks use a distinctive model which is suitable for sequence data. Sequence data can include human speech, natural language, and numerical time series. Natural language applications include machine translation and question response systems. RNNs can also be applied to multi-dimensional data. For example, RNNs are used to caption images.   \n",
    "\n",
    "The idea of RNNs is not new. In the early days of the last surge in interest in neural networks, the basic architecture was proposed by Rumelhart et. al. (1986). The idea remained in limited use, until Hochreiter and Schmidhuber proposed the Long Short Term Memory (LSTM) network in 1997. RNNs remain an active area of research with improvements appearing regularly. \n",
    "\n",
    "In this lesson you will learn:\n",
    "\n",
    "1. The basic concepts of RNNs. \n",
    "2. An introduction to text processing and feature embedding methods.\n",
    "3. How RNNs can be used for sequence genertion. \n",
    "4. Some options for deep RNN architectures.\n",
    "5. How RNN architectures can employ memory to work over different time scales. \n",
    "6. Methods for regularization of RNNs.\n",
    "7. How the  bidirectional RNN architecture works. \n",
    "\n",
    "****\n",
    "Required readings from the GBC book are:\n",
    "- 10.0\n",
    "- 10.1\n",
    "- 10.2\n",
    "- 10.2.1; not covered in this lesson\n",
    "- 10.2.2; not covered in this lesson\n",
    "- 10.2.3; optional, not covered in this lesson\n",
    "- 10.2.4\n",
    "- 10.3\n",
    "- 10.4; optional\n",
    "- 10.5\n",
    "- 10.7\n",
    "- 10.10\n",
    "- 10.10.1\n",
    "- 10.10.2\n",
    "- 10.11\n",
    "- 10.11.1\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic recurrence relationships for neural networks\n",
    "\n",
    "**Recurrent neural networks (RNNs)** use recurrence operations to operate on **sequences**. But, what do we mean by a recurrence? Simply put, recurrent functions are functions that call themselves to process values of a sequence. \n",
    "\n",
    "Let's look at a simple example. Start with a function that operates on a value at one time step to compute an output at the next time step. We can write such a function as follows:\n",
    "\n",
    "$$s^{(t)} = f(s^{(t-1)}; \\theta)$$\n",
    "\n",
    "This apply this functional relation once again applied again to compute the output at the next time step. This first recurrent relationship can be written as follows.\n",
    "\n",
    "$$s^{(t+1)} = f(f(s^{(t-1)}; \\theta); \\theta)$$\n",
    "\n",
    "We can continue in this way to compute the output of any **finite** sequence of inputs. It is important to understand that this recurrence reltionship in not defined for an infinite length sequence. In paractice, all problems for which we apply the recurrence relationship are finite. \n",
    "\n",
    "Notice that the recurrence relationship intorduces **memory**. The memory property of this model differentiates it from many classical time series models. In technical terms we can say the recurrence model is **non-Markovian**, where a **Markov process** is memoryless.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basic recurrent neural network architecture\n",
    "\n",
    "How do we create a neural network with a recurrence relationship? A single hidden layer recurrent neural network is illustrated in Figure 1.1 below. The hidden layer of this network has a recurrent or feedback path. This RNN produces an output value (or vector) for each input.     \n",
    "\n",
    "<img src=\"img/Recurrent1.jpg\" alt=\"Drawing\" style=\"width:200px; height:400px\"/>\n",
    "<center>**Figure 1.1. Basic architecture of a recurrent process**</center>\n",
    "\n",
    "Figure 1.1 illustrates the concept of an RNN. But, how can this architecture be used to perform actual compuations? The answer is to **unfold** the recurrence relationship. This unfolding is illustrated in Figure 1.2. Notice that the unfolded network represents a long sequence of calculations. \n",
    "\n",
    "<img src=\"img/Unfolded1.jpg\" alt=\"Drawing\" style=\"width:450px; height:400px\"/>\n",
    "<center>**Figure 1.2. Unrolled basic recurrent process**</center>\n",
    "\n",
    "The capital letters in Figure 1.2 represent weight tensors of the model. It is these weight tensors that must be learned when training an RNN. Typcially, these weight tensors are learned by using **back propagation through time (BPTT)**. BPTT is a generalization of the general back propagation algorithm. \n",
    "\n",
    "The weight tensor $W$ defines the recurrence. You can see that this connection from one time step to another introduces memory into the model. \n",
    "\n",
    "Given these weight tensors the feed-forward calculation for a multinomial classifier RRN can be writen as follows:  \n",
    "\n",
    "$$a^{(t)} = b + W h^{(t-1)} + Ux^{(t)}\\\\\n",
    "h^{(t)} = tanh(a^{(t)})\\\\\n",
    "o^{(t)} = c + Vh^{(t)}\\\\\n",
    "\\hat{y}^{(t)} = softmax(o^{(t)})$$\n",
    "\n",
    "These relationships are relatively complex. This is especially the case when comparied to basic fully-connected networks. \n",
    "\n",
    "In the equations above notice that there is only a dependency on the previous activation, $h^{(t-1)}$, and the current input, $x^{(t)}$. There is no dependency on anything that occurs after the current time step. Therefore, we can stay that this basic recurrent neural network is **causal**. For many problems in sequence analysis causality is an essential property. For example, in a forecasing application there will never be any information available beyone the present. \n",
    "\n",
    "Notice that the weight tensors of the RNN are the same at each time step. This fact has two important implications.\n",
    "1. These **Shared weight** tensors are learned. This makes the training of RNNs tractable. Furhter, the statistical strenght of the training is increased. \n",
    "2. The RNN can only be used to model **stationary** sequences. A stationary sequence has statistical properties (e.g. variance) that are **invariant with time**.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Recurrent network with single output\n",
    "\n",
    "The RNN architecture illustrated in Figure 1.3 has only one output that is generated only once the end of the sequence is encountered. This type of RNN can be used in a number of ways. For example, the single output RNN can be used as a trigger that detects a certain patern in a sequence. In can also be used as a classifier for the input sequence.   \n",
    "\n",
    "<img src=\"img/RNNwithSingOutput.JPG\" alt=\"Drawing\" style=\"width:500px; height:400px\"/>\n",
    "<center>**Figure 1.3. RNN with single output**</center>\n",
    "\n",
    "A common issue with single output RNNs is difficulty with training. BPTT requires a gradient based on the partial derivatives of the loss function weights with respect to the weights. This gradient can only be computed at the end of each sequence. This situation can greatly slow training if the sequences are long or arrive infrequenty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Text example with Keras\n",
    "\n",
    "Let's see how some of this theory works in practice using Keras. Execute the code in the cell below to load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Overview of text embedding\n",
    "\n",
    "In the following examples we will use a feature extraction technique known as **text embedding**. A text embedding layer is often used to extract features before RNN layers are used. This general architecture is shown in Figure 2.1 \n",
    "\n",
    "<img src=\"img/Architecture.JPG\" alt=\"Drawing\" style=\"width:250px; height:250px\"/>\n",
    "<center>**Figure 2.1. General architecture for deep learning with text**</center>\n",
    "\n",
    "We will not dive deeply into NLP for this lesson since our focus is on RNNs. However, a bit of background will help you understand what is going on.\n",
    "\n",
    "First a bit of terminology. We talk about a **corpus**, which a collection of **documents**. A document can take many forms; a email, a tweet, a news artical, a book, a chapter from a book, a paragraph, etc. \n",
    "\n",
    "There are a great many models which have been tried for text analytics and **natural language processing (NLP)**. Typically, NLP with raw text requires significant preparation.  A few of many possible preparation steps include: \n",
    "- Text is normalized, which can involve many steps such as setting characters to lower case, removing puntuation, numbers and special characters. In some cases, **stop words** are removed. Stop words are common words with no sematic value, such as 'the' or 'and'. \n",
    "- Normalized text is **tokenized**. There are a number of ways to tokenize text. Most coommonly the tokens are the words in the document. Tokens can also be characters or any other division of the document that make symantic sense. \n",
    "\n",
    "We will only discuss a few of common models briefly here: \n",
    "\n",
    "- The **bag of words model (BOW)** is a simple widely used and suprsingly effective model for analysis of text data. The BOW model assumes **exchangeability** of words. The BOW model depends only on the frequency of the words in the document. The order of the words is not consisdered. Dispite these seamingly rediculous assumptions, the model wo rks well in many cases. The end product of applying the BOW model is a term-document or document-term matrix. The tdm, or dtm is a structured representation of word frequency by document. The tdm or dtm can be used for classification if lables are available or clustering for unspervised learning. In general the tdm or dtm are extremely sparse. Most words in a typical corpus do not show up in most documents. This sparscesity requires that special data structures are used for storage and manipulation. Further, the sparse representation leads to statistical issues. \n",
    "- The **N-gram** model uses all possible sequences of N words. For example, the 3-gram of the sentance 'The cat sat on the mat' is as follows:\n",
    " - All possible 1-grams; the, cat, sat, on, mat. Note that the 1-gram model is close to the BOW model. \n",
    " - All possible 2-grams; the cat, cat sat, sat on, on the, the mat. \n",
    " - All possible 3-grams; the cat sat, cat sat on, sat on the, on the mat. \n",
    "- Another widely used model is **Part of Speech (PoS) Tagging**. PoS tagging attempts to label or anotate words in a corpus (e.g. a collection of documents) as, say nouns, verbs, pronouns, etc. PoS tagging is beyond the  scope of this Lesson. The PoS tagger creates a tree of the relationship of words in say a sentance. One useful specialization of PoS tagging is named entity recognition, which attempts to find proper nouns. \n",
    "\n",
    "With the advent of deep learning, text embedding models have come into common use. Embedding models map the tokens (typically words) in the documents to a dense space of learned features. The dense represenation has significant statisticall advantages when training models. Further minimal storage is requred for the dense representation. \n",
    "\n",
    "Similar words lie near each other in the embedding space in a maner that preserves symantic relationships. This relationship between some words is shown scematically in Figure 2.2. You can see that simplar words are close to each other. Further, words with similar sematic relationships are along nearly parallel vectors in this space.    \n",
    "\n",
    "![](img/Embedding.JPG)\n",
    "<center>**Figure 2.2. Schematic diagram of embedding relationship between common words**</center>\n",
    "\n",
    "Text embedding has been shown to work extremely well in practice. Unfortunately, like many topics in deep learning, theory has not caught up with practice. Beyond the conceptual level theory is sketchy. The most readable source on the theory of embedding is the [article by Rong](https://arxiv.org/pdf/1411.2738.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Simple text embedding example\n",
    "\n",
    "Let's try a simple sentiment classifier using text embedding. We will try sentiment analysis of movie review text using the IMDB dataset built into Keras. This dataset includes the full text of a number of movie reviews, along with labels indicating if the review has positive of negative sentiment. \n",
    "\n",
    "***\n",
    "**Note.** A sentiment analysis example is not ideal to demonstrate appling RNNs to text. Most likely, this fact arrises from the fact that word order of the squence is less important in this applicaiton. \n",
    "***\n",
    "\n",
    "The code in the cell below performs the following steps:\n",
    "1. Loads the IMDB data into train and test subsets.\n",
    "2. Pads the squence of words in each review so that all sequences have the same length. In this case, the first 250 words are used. Shorter reviews are padded with zeros. Padding is required so that Keras neural networks can operate on the input. \n",
    "3. Prints some summary information on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 250\n",
    "(train_text, train_labels), (test_text, test_labels) = imdb.load_data(num_words = max_features)\n",
    "train_text = preprocessing.sequence.pad_sequences(train_text, maxlen = max_len)\n",
    "test_text = preprocessing.sequence.pad_sequences(test_text, maxlen = max_len)\n",
    "print(len(train_text))\n",
    "print(train_text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus contains 25,000 documents, or movie reviews in the training dataset. The encoding of the second review is printed. The first many entries in the list are 0, which is the padding. The integers are used to encode each word. The integer encoding creates a data structure suitable for use with neural networks or other machine learning. The assignment of these codes in arbitrary.  \n",
    "\n",
    "Now, you will create a basic (not RNN) neural network model to classify the sentiment of these reviews. The code in the cell below does the following:\n",
    "\n",
    "1. Creates a Keras sequential model following the usual recipe. \n",
    "2. The first 10,000 documents are used to create an embedded feature space of dimension 8. \n",
    "3. The embedded represenation is flattened. \n",
    "4. A fully-connected or dense layer is used as a binary classifier for the sentiment, positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Sequential()\n",
    "## First add an embedding layer\n",
    "embedding.add(Embedding(10000, 8, input_length = max_len))\n",
    "## Flatten the embedding of the features\n",
    "embedding.add(Flatten())\n",
    "## Now the  binary classifier layer\n",
    "embedding.add(Dense(1, activation = 'sigmoid'))\n",
    "embedding.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the embedding layer has many times more parameters than the fully connected output layer. This situation is typical with these types of architectures. \n",
    "\n",
    "Next, execute the code in the cell below to fit or train this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyEMB = embedding.fit(train_text, train_labels,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 256,\n",
    "                   validation_data = (test_text, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fairly naive model works suprisingly well. The result is far from state-of-the-art, but not terrible. \n",
    "\n",
    "Now, execute the code in the cell below to examine the histor of loss vs. epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    '''Function to plot the loss vs. epoch'''\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(test_loss) + 1))\n",
    "    plt.plot(x, test_loss, color = 'red', label = 'Test loss')\n",
    "    plt.plot(x, train_loss, label = 'Training losss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    \n",
    "plot_loss(historyEMB)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function appears relatively well behaived. The loss decreases rapidly and then plateaus. \n",
    "\n",
    "Run the code below to examine the accuracy vs. epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    train_acc = history.history['acc']\n",
    "    test_acc = history.history['val_acc']\n",
    "    x = list(range(1, len(test_acc) + 1))\n",
    "    plt.plot(x, test_acc, color = 'red', label = 'Test accuracy')\n",
    "    plt.plot(x, train_acc, label = 'Training accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Epoch')  \n",
    "    \n",
    "plot_accuracy(historyEMB) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of accuracy vs epoch is similarly well behaived like the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RNN applied to embedded space\n",
    "\n",
    "Let's try the same experiment, but using an RNN layer. The code in the cell below is nearly identical to the code used in the first example, except that a `SimpleRNN` layer has been added between the embedding layer and the fully-connected layer. Execute this code and observe the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RNN1 = Sequential()\n",
    "## First add an embedding layer\n",
    "RNN1.add(Embedding(max_features, 32))\n",
    "## Now add an RNN layer\n",
    "RNN1.add(SimpleRNN(32, kernel_regularizer = regularizers.l2(0.005)))\n",
    "## And the classifier layer\n",
    "RNN1.add(Dense(1, activation = 'sigmoid'))\n",
    "RNN1.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "RNN1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has significantly greater complexity than the first model. The embedding layer is now producing a sequence of 32 for the RNN layer. This leads to many more weights or parameters. Notice that the output or classifier layer is now quite a bit simpler as it only has 32 inputs and weights. \n",
    "\n",
    "Execute the code in the cell below to train this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "historyRNN = RNN1.fit(train_text, train_labels,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 1024,\n",
    "                   validation_data = (test_text, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and accuracy achieved with the RNN layer is similar to, but not quite as good as the first model. For the application the recurrence is evidently not helpful. \n",
    "\n",
    "Execute the code in the cell below and examine the loss vs. epoch for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(historyRNN)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function looks rather erratic across the epochs. This may (or may not) be a sign that the training of this model with so many parameters is struggling. \n",
    "\n",
    "Execute the code in the cell below ad examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy(historyRNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with loss, accuracy shows quite erratic behavior with epoch. The training of  this model may be struggling which helps explain the lackluster perforance. \n",
    "\n",
    "***\n",
    "**Note** For production work with embedding layers, pretrained models are generally used as a starting point. These models have been trained on a wide range of corpora. Therefore, these models are likely to be more powerful and less susceptible to unexpected behavior when faced with new input sequences. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Sequence generation with RNNs\n",
    "\n",
    "There are many interesting applications which generates a sequnce given a, possibly vector, input. A few of the applications of such a system include:\n",
    "- A speach generation for a question-response system, where the output sequence is a response to a question type.\n",
    "- Generating the words of image captions given the class of the image.  \n",
    "\n",
    "A schematic architecture for a sequence generation RNN is shown in Figure 3.1 below. This is a simple example of a **generative model**. That is, it generates a squence in response to the input. We can understand the operation of this network as follows:\n",
    "1. The network has a single input, $x$. \n",
    "2. The activations at a step $t$ in the sequence $h^{(t)}$ are determined by the value of $x$ and the expected value of the output $y^{(t)}$.\n",
    "3. The loss function at each time step $t$, $L^{(t)}$, has two arguments $y^{(t-1)}$ and $\\hat{y}^{(t)}$. \n",
    "\n",
    "<img src=\"img/SequenceGenRNN.jpg\" alt=\"Drawing\" style=\"width:475px; height:400px\"/>\n",
    "<center>**Figure 3.1. Sequence generation with RNN**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Adding depth to RNNs\n",
    "\n",
    "The RNN architectures we have discussed so far in this lesson have been shallow with only a single recurrent hidden layer. Having a single hidden layer limits the representation capacity of the RNN. It is an attractive concept to add additional layers \n",
    "\n",
    "To overcome the limitations of a single hidden recurrent layers, deep architectures can be used in principle. There are serveral possibilites for adding depth to the basic RNN architecture. While these ideas are not new, none is completely satisfactory, and research on alternatives continue.\n",
    "\n",
    "One possibible archietecture is shown in Figure 4.1. This architecture takes a simple approach of stacking recurrent layers. This architecture can be effective in some cases, but suffers from being difficult to train, particularly with both depth and many time steps. \n",
    "\n",
    "<img src=\"img/DeepRNN1.jpg\" alt=\"Drawing\" style=\"width:200px; height:400px\"/>\n",
    "<center>**Figure 4.1. Basic deep RNN**</center>\n",
    "\n",
    "Another possibility is to add another neural network in the recurrence loop is shown in Figure 4.2. The chosen depth and breath of this additional network can add significant capacity to the model. \n",
    "\n",
    "The delay caused by the additional neural network affects the behavior of the recurrence relationship. A solution to this problem is to add the **skip loop** shown in Figure 4.2. The skip loop makes the recurrence responsive to each time step. The skip loop is essentially just another memory, with another assoicated set of weights. \n",
    "\n",
    "<img src=\"img/DeepRNNSkip.jpg\" alt=\"Drawing\" style=\"width:250px; height:400px\"/>\n",
    "<center>**Figure 4.2. RNN with skip connection**</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other deep RNN architectures the ones shown in Figures 4.1 and 4.2 are not ideal.  Training can prove to be slow and difficult as we will discuss in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 RNNs with memory\n",
    "\n",
    "In this section we will explore **long-short term (LSTM)** neural networks and a related architecture known as **gated recurrent units (GRU)**. These architectures are the backbone of many real-world deep learning applications. LSTM and GRU networks are used for machine translation, NLP,and speach recognition to name just a few. In fact, virtually all large-scale production applications of RNNs use some kind of memory architecture. \n",
    "\n",
    "LSTM and GRU networks have proven farily easy to train.  This is in contrast to simple recurrent neural networks, which can prove difficult to train. The difficulty training is generally the result of the **vanishing gradient** and **exploding gradient** problems discussed in the optimizatiton lesson. To gain some insight into why this might be, consider the recurrence relationship with a single input, $s^{(t)}$, after $n$ time steps:\n",
    "\n",
    "$$s^{(t+n)} = f(f(....f(f(s^{(t)}; \\theta));\\theta)$$\n",
    "\n",
    "Now if maginitude of $f(s; \\Theta)$ is greater than $1.0$ the output value will increase exponentially, leading to an exploding gradinet after a long sequence. Conversly, if the maginitude of $f(s; \\Theta)$ is less than $1.0$ the output value will decrease with an exponential decay, and the gradient will vanish after a long sequence. \n",
    "\n",
    "What can be done to correct the aforementioned situation? One effective solution is to add memory to the RNN. The memory makes the RNN sensitive to multiple time scales. Along with the memory as series of **forget gates** are added. Forget gates allow the the network to operate with multiple time scales. Limiting memory time limits the number of time periods for recurrence which limits vanishing and exploding gradients.  \n",
    "\n",
    "A readable summary and comparison of LSTM and GRU architectures can be found in the [paper by Chung et. al.](https://arxiv.org/abs/1412.3555). Chung et. al. compare the performance of both architectures for serveral tasks and datasets and find that which works better is dataset and task dependent.\n",
    "\n",
    "### 5.1 LSTM networks\n",
    "\n",
    "An example of a neural network with memory and forget gates, known as an LSTM, is illustrated in Figure 5.1 below. This architecture was originally proposed by Hochreiter and Schmidhuber in 1997. This architecture uses a series of gates (multiplers with inputs of approximately $0$ or $1$). The memory is shown as the shaded block of the self loop. The other memory loop is shown in green. The purpose of the second memory loop is to control the activations of the forget gates. \n",
    "\n",
    "![](img/LSTM1.jpg)\n",
    "<center>**Figure 5.1. Architecture of long-short term network**</center>\n",
    "\n",
    "There are quite a few forget gates in the architecture shown in Figure 5.1. The activation or output from the network can be written:\n",
    "\n",
    "$$\\hat{y}^{(t)}_i = \\sigma^{(t)}_i \\cdot tanh(y^{(t)}_i)$$\n",
    "\n",
    "Notice that the nonlinearity for the activation is given by the $tanh$ function. The gating function is determined by a sigmoid function:\n",
    "\n",
    "$$\\sigma^{(t)}_i =  \\sigma \\big(V_{(o)} s^{(t)}_i + U_{(o)} x_i^{(t)} + W_{(o)} h^{(t-1)}_i \\big)$$\n",
    "\n",
    "The **state** of the memory loop is a linear combination of the input and the memory. This relationship can be expressed as:\n",
    "\n",
    "$$s^{(t)}_i = f^{(t)}_i \\cdot s^{(t)}_i + i^{(t)}_i \\cdot s^{(t-1)}_i$$\n",
    "\n",
    "Where the new memory state is expressed as:\n",
    "\n",
    "$$s^{(t)}_i = tanh(U_{c} x_i^{(t)} + W_{c} h^{(t-1)}_i)$$\n",
    "\n",
    "The activations of the forget gate and input gate are expressed the the sigmoidal functions:\n",
    "\n",
    "$$f^{(t)}_i = \\sigma \\big(V_{(f)} x^{(t)} + U_{(f)} x_i^{(t)} + W_{(f)} h^{(t-1)}_i \\big)\\\\\n",
    "i^{(t)}_i = \\sigma \\big(V_{(i)} x^{(t)} + U_{i} x_(i)^{(t)} + W_{(i)} h^{(t-1)}_i \\big)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 GRU networks\n",
    "\n",
    "The numerious forget gates in the LSTM network leads to some complexity in understanding and in training. The GRU architecture by Cho et. al. in 2014 as a possible simplification of a RNN with multiple time scales. The GRU architecture does not require an explicit memory unit. Instead, GRUs use a weighted decay function for the output activation:\n",
    "\n",
    "$$h_i^{(t)} = \\big( 1 - z _i^{(t)} \\big) h_i^{(t-1)} + z _i^{(t)} \\tilde{h}_i^{(t-1)}$$\n",
    "\n",
    "The schematic architecture of a GRU network is shown in figure 5.2. Notice that there are two loops, but no memory. This arrangement effectively implements the relationship shown in the equation above. \n",
    "\n",
    "![](img/GRU.JPG)\n",
    "<center>**Figure 5.2. Schematic architecture of GRU network**</center>\n",
    "\n",
    "The update gate is controled by the a sigmoidal function of the following form: \n",
    "\n",
    "$$z _i^{(t)} = \\sigma \\big( U_{(z)} h_i^{(t-1)} + W_{(z)} x^{(t)}_i \\big)$$\n",
    "\n",
    "The **candidate activation**, $\\tilde{h}_i^{(t-1)}$, is computed in a similar manerr to the typical recurrent unit using the following relationship: \n",
    "\n",
    "$$\\tilde{h}_i^{(t-1)} = tanh \\big( U( r_i^{(t)} \\odot h^{(t-1)}_i) + W_{(z)} x^{(t)}_i \\big)$$\n",
    "\n",
    "The activation of the reset gate are determined by the following relationship:\n",
    "\n",
    "$$r_i^{(t)} = \\sigma \\big( U_{(r)} h_i^{(t-1)} + W_{(r)} x^{(t)}_i \\big)$$\n",
    "\n",
    "You can see from the above equations that the architecture GRU is indeed a simplification when compared to the LSTM architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 A GRU example\n",
    "\n",
    "With the above theory in mind, it is time to put these ideas to practice on an example. The code in the cell below is similar to the foregoing examples, but uses a GRU layer. Notice mild l2 regularization has been specified. Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU1 = Sequential()\n",
    "## First add an embedding layer\n",
    "GRU1.add(Embedding(max_features, 32))\n",
    "## Now add an RNN layer\n",
    "GRU1.add(GRU(32, kernel_regularizer = regularizers.l2(0.005)))\n",
    "## And the classifier layer\n",
    "GRU1.add(Dense(1, activation = 'sigmoid'))\n",
    "GRU1.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "GRU1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU layer has about three times the number of parameters as the simple RNN layer. This should be expected, given the more complex algorithm. Still, because of parameter sharing, this number is not excessive. \n",
    "\n",
    "Now, execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyGRU1 = GRU1.fit(train_text, train_labels,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 1024,\n",
    "                   validation_data = (test_text, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results with the GRU are slightly better than for the simple RNN. \n",
    "\n",
    "Next, execute the code in the cell below to display the plot of loss vs. epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_loss(historyGRU1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to display the plot of accuracy vs. epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(historyGRU1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the test loss and test accuracy show a relatively smooth well-hehaived curve for the first epochs. This is in contrast to the jagged curves obtained for the un-regualized simple RNN tried previously. Or, the difference may just be run-to-run random effects. \n",
    "\n",
    "It is possible that bit more regularization and more training epochs would give better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Regualarization of RNN\n",
    "\n",
    "Like all neural networks regularization is typically applied to RNNs. Both l1 and l2 regularization are commonly used. In fact, the foregoing examples use some mild l2 regularization. \n",
    "\n",
    "However, dropout has proven difficult to deal with. The usual dropout algorithm causes problems with BPTT. This problem arrises because the Bernoulli sampling at each time step causes instability durring BPTT. \n",
    "\n",
    "A solution to this problem has only recently been found by Gal and Ghahramani (2016) and Gal (2016). In these highly technical papers the concept of using dropout as a sampling method for a Gaussian Bayesian model is developed. The solution is to use a special case of dropout in which the Bernoulli sampling is not updated at each forward pass through the network. This method is called **recurrent dropout**. \n",
    "\n",
    "The code in the cell below implements the same model as was tested above, but with the addition of dropout. In fact, two types of dropout are used, regular dropout with parameter 0.01 and recurrent dropout with parameter 0.1. it is common practice to include a small amount of conventional dropout along with the reecurrent dropout. Execute this code. \n",
    "\n",
    "****\n",
    "**Note:** This implementationeof the recurrent dropout algorithm in Keras was done by Gal as part of his thesis reserach. \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RNN2 = Sequential()\n",
    "## First add an embedding layer\n",
    "RNN2.add(Embedding(max_features, 32))\n",
    "## Now add an RNN layer with dropout\n",
    "RNN2.add(GRU(32, dropout=0.01, recurrent_dropout=0.1,\n",
    "            kernel_regularizer = regularizers.l2(0.005)))\n",
    "## And the classifier layer\n",
    "RNN2.add(Dense(1, activation = 'sigmoid'))\n",
    "RNN2.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "RNN2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should come as no suprise that there are same number of parameters for GUR layers with dropout are the same as the layers without dropout. \n",
    "\n",
    "Now, execute the code in the cell below to fit the model and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyRNN2 = RNN2.fit(train_text, train_labels,\n",
    "                   epochs = 15,\n",
    "                   batch_size = 1024,\n",
    "                   validation_data = (test_text, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error and  loss achieved are not as good as with l2 regularization alone. Either the dropout parameters are set too large, or this problem is not amenable to dropout regularization. \n",
    "\n",
    "PLot the curve of loss vs epoch by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(historyRNN2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the coded in the cell below to plot the curve of accuracy vs. epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(historyRNN2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These curves are a bit smoother than before, indicating the dropout regularization is having some effect. However, the training seems to hit the  point of over-fitting rather quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Bidirectional RNNs\n",
    "\n",
    "Up until now we have worked only with RNNs which take steps along a sequence in one direction. As a result, we said these models are causal. However, there are many situations where causality is not important. A few examples include:  \n",
    "- NLP applications, where the order words are processed is not important. \n",
    "- Handwritting recognition, where characters can be recognized in any order. \n",
    "- Speach recognition, where the phenomes of a captured utternance are analyized as a group. \n",
    "- Figure captioning, where the figure can be understood scanning left to right, right to left, top to bottom and bottom to top. In fact, a 4-directional neural network architecture can be applied to this case. \n",
    "\n",
    "Given that order of processing for many sequences does not mater it is possible to go in both directions at once. RNNs that perform such processing are know as **bidirectional RNNs**. An example of a simple bidirectional RNN achitecture is shown in figure 7.1 below. \n",
    "\n",
    "![](img/Bidirectional1.JPG)\n",
    "<center>**Figure 7.1. Architecture of simple bidirectional RNN**</center>\n",
    "\n",
    "It is relatively straight forward to derrive the propagtion equations for  the case of a simple bidirectional RNN. These equations are  shown below. \n",
    "\n",
    "$$a^{(t)} = b + W h^{(t-1)} + Ux^{(t)}\\\\\n",
    "i^{(t)} = d + Q g^{(t+1)} + Rx^{(t)}\\\\\n",
    "h^{(t)} = tanh(a^{(t)})\\\\\n",
    "g^{(t)} = tanh(i^{(t)})\\\\\n",
    "o^{(t)} = c + Vh^{(t)} + Sg^{(t)}\\\\\n",
    "\\hat{y}^{(t)} = softmax(o^{(t)})$$\n",
    "\n",
    "In addition to the usual weight tensors, $W$ and $U$ ,for the forward case, there are corresponding tensors, $Q$ and $R$ for the backward case. The activation of the forward case is given by $h^{(t)}$, and for the backward case, $g^{(t)}$. The output of the network, $\\hat{y}^{(t)}$, is the softmax of the linear combination of these forward and backward activations with weight tensors $V$ and $S$. There bias tensors are required, $c$, $b$, and $d$.\n",
    "\n",
    "By examining the above equations, one can see that bidirectional RNNs are **non-causal**. As already stated, this property may actually be an advantage in many applications. \n",
    "\n",
    "The code in the cell below implements a bidirectional GRU neural network. In Keras, the `bidirectional` operator can wrap most any RNN layer type. This wraper creates the bidirectional version of the specified RNN. In this case we will use a bidirectional GRU. Execute the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirection = Sequential()\n",
    "## First add an embedding layer\n",
    "bidirection.add(Embedding(max_features, 32))\n",
    "## Now add an RNN layer\n",
    "bidirection.add(Bidirectional(GRU(32, kernel_regularizer = regularizers.l2(0.005))))\n",
    "## And the classifier layer\n",
    "bidirection.add(Dense(1, activation = 'sigmoid'))\n",
    "bidirection.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "bidirection.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of parameters for the RNN layer has doubled from the unidirectional GRU example explored earlier. \n",
    "\n",
    "Next, execute the code in the cell below to fit this model and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyBi = bidirection.fit(train_text, train_labels,\n",
    "                   epochs = 15,\n",
    "                   batch_size = 1024,\n",
    "                   validation_data = (test_text, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are slightly better than any of the other RNN architectures tried. Perhaps there is an advantange in the bidirectional architecture for this case.\n",
    "\n",
    "Execute the code in the cell below to plot the loss vs. epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(historyBi)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to plot the accuracy vs. epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy(historyBi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bidirectional network shows slow and erratic convergence towards the best solution. Perhaps a bit more regularization and longer run might help. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 RNNs vs. convolutional neural networks\n",
    "\n",
    "It may have caught your notice that convolutional neural networks could be applied to many of the applications discussed in this lesson. This is in fact the case. For example, a one-dimensional seequence can be modeled by either an RNN or a one-dimensional covnet. \n",
    "\n",
    "In some cases covnets have proved superior. This superior performance likely results from the simpler and easier to train model. However, if the underlying process operates over long time scales, then an RNN with memory will likely be superior, since covnets have no memory. \n",
    "\n",
    "Emperical evidence indicates that one can only decide which architecture is better for a given application by either deep understanding  of the underlying process, actual testing or both. Testing the various purmutations of these two basic architectures can be quite time consumming. However there  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0 Multi-task architectures\n",
    "\n",
    "So far, we have only considered RNN architectures which perform a **single task**. In our examples this task is to estimate sentiment in movie reviews. However, many, if not most, real-world AI applications require more than one task. We say that systems that perform multiple tasks are **multi-task architectures**. Conventionally, these tasks were all performed by independent systems. An advantage of deep architectures is that the multiple tasks can be performed in one stack. Further, the machine learning engineer does not need to explicitly engineer features for these tasks. \n",
    "\n",
    "Let's look at a examples of multi-task architecture. Real time translation of human speach to another language. At the minimum there are  four tasks:\n",
    "- Compute the words in the utterance from the spoken phenomes. \n",
    "- Apply NLP to determine the context of the utterance. \n",
    "- Create the sequuence of words in the other language. \n",
    "- And finally, create the sequence phenomes to form the utterance understandable to the speaker of the other language. \n",
    "\n",
    "In this particular application two of the tasks are predictive. But the two other tasks are **generative**. That is, the purpose of these tasks is to compute an output sequence given an input. \n",
    "\n",
    "Figure 9.0 shows a schematic architecture for multi-task deep learning. In this  case, the end-to-end task is to transforming utterances in one  language to utterences in another language. \n",
    "\n",
    "<img src=\"img/MultTaskLearning.jpg\" alt=\"Drawing\" style=\"width:475px; height:400px\"/>\n",
    "<center>**Figure 9.0. Deep learning multi-task architecture for machine translation**</center>\n",
    "\n",
    "While the tasks are shown as descrete steps, a deep learning  neural network would perform all of of these tasks. This approach has the advantage that training is end-to-end. Thus, the perforance and training loss is measured on the complete complex task not the individual steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
