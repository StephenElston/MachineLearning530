{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Object Recognition\n",
    "\n",
    "## Overview\n",
    "\n",
    "In other lessons we have primarily dealt with classification problems. We have worked with images with a single object which we classify into its category.    \n",
    "\n",
    "**Object recognition** is a more difficult task. The process allows us to deal with complex images containing multiple objects. There are two major sets in this process:    \n",
    "1. **Detect** the presence of each of multiple objects in an image. In other lessons, we have dealt with object classification, where there is a single object of interest in the image. Having an unknown number of objects in the image introduces considerable difficulty in the problem. Another difficulty is the trade-off between correctly detecting the objects of interest and falsely detecting spurious objects in the background.  \n",
    "2. Compute a minimum size **bounding box** around each object. \n",
    "3. The object detected in each of the bounding box is **classified**. \n",
    "\n",
    "Our goal is to give you an understanding of how modern object recognition algorithms work. Given the research momentum in this area it is entirely likely that there will be better performing algorithms by the time you read this. \n",
    "\n",
    "The object recognition problem has been around for quite some time. Historically, object recognition was performed using hand engineered features. By 2012 the **mean average precision** or **mAP** of object recognition had stagnated at around 40%. Only since 2013 there has been a surge of both interest and performance of object recognition with the introduction of deep learning methods. Since that time, the mAP achieved has continued to increase and is now routinely over 80%.  These algorithms no longer require the cumbersome process of hand engineering features. Some key papers in the history of deep object recognition are:\n",
    "\n",
    "1. In 2013 [Erhan et. al.](https://arxiv.org/pdf/1312.2249.pdf) published Scalable Object Detection using Deep Neural Networks, which introduced the R-CNN algorithm the first widely accepted deep learning object detection algorithm. R-CNN demonstrated a significant improvement in object recognition accuracy. However, this algorithm proved to be too slow for real-time video processing. \n",
    "2. The Fast R-CNN algorithm was introduced by [Girshick](https://arxiv.org/pdf/1504.08083.pdf) Fast R-CNN simplified the required computations but still struggled with real-time video.  \n",
    "3. Further improvements by [Ren et. al.](https://arxiv.org/pdf/1506.01497.pdf) lead to the Faster R-CNN algorithm. However, the computational complexity of this algorithm was still rather high.  \n",
    "4. The Mask R-CNN algorithm was proposed by [He, et. al. in 2018](https://arxiv.org/pdf/1703.06870.pdf) The Mask R-CNN algorithm exhibits significantly improved object detection accuracy. This is particularly the case where there are large numbers of objects, such as flock of birds or a crowd of people. While not efficient enough for real-time video, Mask R-CNN should be considered if accuracy in complex scenes is required. \n",
    "\n",
    "All of these algorithms share a similar architecture. This architecture is in the form of a pipeline with process steps computed sequentially. The key steps in these pipelines generally include: \n",
    "\n",
    "1. **Convolutional Neural Network:** The CNN creates features which which are used to detect and then classify objects in the image.   \n",
    "2. **Candidate bounding boxes:** Candidate bounding boxes are generated. Multiple candidate bounding boxes cover each region. \n",
    "3. **Filtering of bounding boxes:** The probability of an object being in each bounding box is computed and low probability boxes are filtered. \n",
    "4. **Minimal bounding boxes:** The size of the bounding boxes must be adjusted to best fit the objects detected. \n",
    "5. **Classification:** The objects in the bounding box are classified. \n",
    "\n",
    "The result is that training and performing inference with these complex pipelines has significant computational complexity. Further, these pipelines can be difficult to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The YOLO Algorithms\n",
    "\n",
    "In this lesson we will not focus on the R-CNN family of algorithms. Instead, we will focus on the You Only Look Once or YOLO algorithms. YOLO was first introduced in by [Redmon et. al.](https://arxiv.org/pdf/1506.02640v5.pdf) in 2016. \n",
    "\n",
    "The YOLO algorithms exhibit slightly less accuracy than say Fast R-CNN and its derivatives. However, YOLO is considerably more efficient and can truly be applied to real-time video. At the time of this writing the YOLO family of algorithms are generally considered to be the state of the art in real-time object recognition. \n",
    "\n",
    "YOLO does have some limitations. For example, it is known that YOLO algorithms do not perform well when faced with a large number of small objects, such as a flock of birds. Mask-RNN algorithms exhibit considerably better performance in these cases, but at a cost in speed.      \n",
    "\n",
    "There are two significant breakthroughs with YOLO:\n",
    "1. The recognition that object detection and classification can be performed in one pass. Hence the name You Only Look Once. All of the R-CNN algorithms required multiple passes to perform object recognition. In some cases several thousand passes are required, which was computationally intensive. \n",
    "2. YOLO is trained as a single model. R-CNN required the training of multiple models, for recognition, bounding box,and classification. To be fair, Fast R-CNN reduced the number of models to be trained to two.\n",
    "\n",
    "Since the original YOLO paper there have been two improved versions of the YOLO algorithms have been published. The latest information on YOLO can be found on the [YOLO web pages](https://pjreddie.com/darknet/yolo/), which is part of the Darknet repository. The [YOLO9000 algorithm](https://arxiv.org/pdf/1612.08242.pdf), which is often referred to as YOLO v2, is a significant improvement over the original YOLO algorithm. Lately, some incremental improvements have been published as [YOLO v3](https://pjreddie.com/media/files/papers/YOLOv3.pdf).\n",
    "\n",
    "\n",
    "********\n",
    "**Note:** As has already been stated, deep object detection is an active area of research. Another computationally efficient algorithms [SSD:  Single Shot MultiBox Detector was proposed by Liu et. al. in 2016](https://arxiv.org/pdf/1512.02325.pdf). \n",
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding box parameterization\n",
    "\n",
    "Object detection requires that we enclose each object with a bounding box. The parameters of the bounding box are computed using a regression model. The model is trained by minimizing the RMSE between the human marked boxes in the training data and the computed bounding boxes. \n",
    "\n",
    "To apply the regression model, we need a parameterization for the bounding box. We do this with respect to **anchor boxes** which are **prior bounding box proposals**. The anchor boxes are transformed by both translation and scaling so that they best fit the object being detected.  \n",
    "\n",
    "Naively, we could simply parameterize the bounding box with a linear function: \n",
    "\n",
    "\\begin{align}\n",
    "b_x &= t_x + c_x\\\\\n",
    "b_y &= t_y + c_y\\\\\n",
    "b_w &= p_w * t_w\\\\\n",
    "b_h &= p_h * t_h\n",
    "\\end{align}\n",
    "\n",
    "Where,  \n",
    "$b_x$ is the x coordinate of the center of the bounding box,   \n",
    "$b_y$ is the y coordinate of the center of the bounding box,  \n",
    "$b_w$ is the width of the bounding box,   \n",
    "$b_h$ is the height of the bounding box,   \n",
    "$c_x$ is the prior of the x coordinate of the bounding box,   \n",
    "$c_y$ is the prior of the y coordinate of the bounding box,     \n",
    "$b_w$ is the prior of the width of the bounding box, and     \n",
    "$b_h$ is the prior of the height of the bounding box.    \n",
    "\n",
    "There are 4 spatial parameters along with the probability that an object is in the box. For each bounding box the parameterization is:   \n",
    "\n",
    "\\begin{align}\n",
    "b_x &= \\sigma(t_x) + c_x\\\\\n",
    "b_y &= \\sigma(t_y) + c_y\\\\\n",
    "b_w &= p_w e^{t_w}\\\\\n",
    "b_h &= p_h e^{t_h}\\\\\n",
    "p_0 &= Pr(object) * IoU(b,\\ object) = \\sigma(t_0)\n",
    "\\end{align}\n",
    "\n",
    "$p_0$ is the probability of an object in the box,\n",
    "\n",
    "These relationships are illustrated in the figure below. The \n",
    "\n",
    "<img src=\"img/BoundingBox1.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>**Proposal bounding box in blue and optimized bounding box in red**</center>\n",
    "\n",
    "The above relationships include priors, often known as proposals, for the parameters of the bounding box. These are the initial values of for the regression and are illustrated in the figure above in blue.      \n",
    "\n",
    "\n",
    "In the above figure the following notation is used to parameterize the best fit bounding box, which is shown in red: \n",
    "$b_x$ is the x coordinate of the center of the bounding box,   \n",
    "$b_y$ is the y coordinate of the center of the bounding box,  \n",
    "$b_w$ is the width of the bounding box,   \n",
    "$b_h$ is the height of the bounding box, and   \n",
    "$p_0$ is the probability of an object in the box.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection by regression\n",
    "\n",
    "Now that we have a parameterization of the object detection problem let's formulate the regression model. Using this parameterization, the best fit bounding box is computed using a regression model. Additionally, we need to classify the object in the bounding box. Our goal is to compute the smallest bounding box that encloses an object along with the probability there is an object in the box and the class of the object. By using a regression model to estimate all of the parameters \n",
    "\n",
    "Training the regression model uses labeled or known cases. The known cases typically have human marked bounding boxes around known objects. The presence and class of the object are also labeled.  \n",
    "\n",
    "To illustrate the concepts we will start with just a single bounding box with three possible object classes. The vector of parameters to be estimated in this regression problem can be written as follows:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = \n",
    "\\begin{bmatrix}b_x \\\\ b_y \\\\ b_w \\\\ b_h \\\\ p_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The first 5 parameters in this vector have already been discussed. The values of $\\{ c_1, c_2, c_3 \\}$ are binary, and indicate which class the object falls in. Typically, the regression model is trained by minimizing the RMS error with the labeled values of the parameters. \n",
    "\n",
    "Next, let's expand the simple single bounding box model to multiple bounding boxes. An example of multiple bounding box proposals sharing a single center, or **anchor**, are shown in the figure below. An object could have various shapes, resulting in a higher probability for one of the bounding boxes enclosing the object than the others.     \n",
    "\n",
    "<img src=\"img/MultiBox.jpg\" alt=\"Drawing\" style=\"width:750px; height:500px\"/>\n",
    "<center>**Bounding boxes with common center or anchor point**</center>\n",
    "\n",
    "There are two key points to note about the figure above.\n",
    "1. The image is subdivided into a 3x3 grid. Bounding boxes are anchored on this grid.\n",
    "2. For each anchor on the grid, there are three bounding boxes. The 3 bounding boxes for two anchors are shown. \n",
    "\n",
    "In the case illustrated, there are 3 bounding box per-grid element for a total of 3x3x3 bounding box proposals. The YOLO algorithm typically uses a 9x9 grid with 5 bounding boxes per grid element, for a total f 9x9x5.  \n",
    "\n",
    "We can generalize the vector of model parameters discussed earlier for a single bounding box. For our example case, we concatenate 3x3x3 sets of parameters, one for each bounding box. An example of concatenating parameters for multiple bounding boxes is shown below.     \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = \n",
    "\\begin{bmatrix}b_x \\\\ b_y \\\\ b_w \\\\ b_h \\\\ p_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\\\ \\vdots \\\\\n",
    "b_x \\\\ b_y \\\\ b_w \\\\ b_h \\\\ p_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "As has already mentioned, the YOLO algorithm there are 9x9x5 bounding box proposals. If there are 80 classes of objects, plus a background class, then there are 9x9x5x81 model parameters.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering bounding boxes\n",
    "\n",
    "The result of the regression model are a large number of bounding box proposals. We need an efficient algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU\n",
    "\n",
    "Object detection requires a metric to determine how well the computed bounding box fits the object. The metric we use is known as Intersection over Union or IoU. This metrics is used to compare human marked bounding boxes with the ones computed by the algorithm. \n",
    "\n",
    "The concept of IoU is illustrated in the figure below:\n",
    "\n",
    "<img src=\"img/IoU.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>**Intersection and union of bounding boxes**</center>\n",
    "\n",
    "Given the two bounding boxes the IoU is easily computed as:\n",
    "\n",
    "$$IoU = \\frac{Area\\ of\\ intersection}{Area\\ of\\ union}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-max Suppression\n",
    "\n",
    "The result of the regression produces the optimal dimensions of the bounding boxes, the probabilities and the classification of the object. There are multiple bounding boxes covering the same area of the image. Therefore, multiple bounding boxes will overlap with each object detected. We need a way to find a single best bounding box for each object. The **non-max suppression algorithm** is just such an algorithm. \n",
    "\n",
    "The non-max suppression algorithm proceeds in two steps:\n",
    "1. The probabilities of objects in each of the boxes is computed.\n",
    "2. Boxes with probabilities below some threshold, say 0.5, are eliminated from consideration.\n",
    "3. For the remaining boxes:\n",
    "  - Select the remaining boxes with the highest probability. \n",
    "  - Compute the IoU for overlapping bounding boxes. \n",
    "  - Filter out bounding boxes with IoU above some threshold, such as 0.6.\n",
    "4. Repeat step 3, until only one box per object remains. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training YOLO\n",
    "\n",
    "Training YOLO requires some special approaches. In this section, we will summarize two of these.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine grain features\n",
    "\n",
    "Good object recognition requires good feature generation. As has already been mentioned, the application of convolutional neural networks for feature generation in object recognition was a major breakthrough. A significant issue with feature generation is that the objects in images can occur at different scales.\n",
    "\n",
    "The YOLO CNN produces a 13x13 feature grid. This grid is adequate for larger scale objects. However, for fine grain objects this grid is too course. However, for finer grain features this grid is not adequate. YOLO solves this problem by using a 26x26 feature grid from higher in the         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-scale feature generation with CNNs\n",
    "\n",
    "Good object recognition requires good feature generation. As has already been mentioned, the application of convolutional neural networks for feature generation in object recognition was a major breakthrough.   \n",
    "\n",
    "A significant issue with feature generation is that the objects in images can occur at different scales. A recent advance in object recognition is the application of multi-scale features using the hierarchy of RNNs. This method was first introduced by [Lin et all](https://arxiv.org/pdf/1612.03144.pdf) in 2017. \n",
    "\n",
    "We have discussed RNNs in another lesson. In that lesson we saw how the scale of the features changes  The heirarchy of RNN layers creates features features \n",
    "\n",
    "<img src=\"img/multiscale.jpg\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center>**Multi-scale feature pyramid**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with joint datasets\n",
    "\n",
    "A significant issue encountered when training a deep neural network to detect and classify objects is the limited detection training data cases. Typically, detection cases also include category labels. Classification cases are much more numerous, but only have category labels. This situation has the advantage that training can include a greater number of categories. However, there is a significant imbalance between the number of object classification only cases and the number of detection cases available for training. \n",
    "\n",
    "Jointly training with a combined dataset requires that a two component loss function be used. One component of the loss measures the classification accuracy. The other component measures the accuracy of location using IoU. In practice, the detection cases are oversampled, but an imbalance still remains. Training with the combined data set results in a stronger detector. This situation leads to two possible training modes:\n",
    "1. The full loss functions is used for training cases with both location and category labels.\n",
    "2. The classification component of the loss function is used for training cases with only category labels. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Classification\n",
    "\n",
    "There is an additional problem which must be addressed when training with combined datasets. The category labels in classification datasets are much more detailed than for detection datasets. For example, the detection dataset might have a category label of simply 'dog'. Whereas, a classification dataset might have a category label of German Shepard Dog, which is much more detailed. \n",
    "\n",
    "A simple object labeling scheme is flat. An example is shown in the figure below.  \n",
    "\n",
    "<img src=\"img/flatlabels.jpg\" alt=\"Drawing\" style=\"width:700px; height:125px\"/>\n",
    "<center>**Flat feature labels**</center>\n",
    "\n",
    "In the above figure notice that all labels are the the same level, regardless of complexity. During classification, an object is placed in one category or another. Such classification is typically done using \n",
    "\n",
    "To integrate datasets with different levels of detail a **hierarchical label** scheme is employed. More specifically, YOLO employees a simplified version of the **WordNet** hierarchy for naming objects. You can find a detailed description of this structure in [Introduction to WordNet: An On-line Lexical Database by Miller, et. al., 1993](http://wordnetcode.princeton.edu/5papers.pdf).\n",
    "\n",
    "An example of a simplified hierarchical label scheme is shown in the figure below.   \n",
    "\n",
    "<img src=\"img/hierarchicallabels.jpg\" alt=\"Drawing\" style=\"width:700px; height:250px\"/>\n",
    "<center>**Hierarchical feature labels**</center>\n",
    "\n",
    "Using the above hierarchical scheme classification can be done at any level in the hierarchy. Cases with complete labels can use the hierarchy from top to bottom. An example might be {object, animal, mammal, dog, German Shepard}. Cases with simple labels can use the hierarchy to the greatest depth possible. An example might be {object, animal, mammal, dog}. This flexibility allows integration of training cases with different levels of detail in the labels. \n",
    "\n",
    "The probability of an object in the hierarchy is the product of the conditional probabilities going from the bottom toward the root. For example, the probability that an object is a German Shepard is computed as follows:\n",
    "\n",
    "$$\n",
    "p(German\\ Shepard) = p(German\\ Shepard\\ |\\ dog) * p(dog\\ |\\ mammal) * p(mammal\\ |\\ animal) * p(animal\\ |\\ physical object)\n",
    "$$\n",
    "\n",
    "In may be the case that the classifier cannot make a reasonably probable identification of the breed of dog. In which case the probability that the object is a dog is:\n",
    "\n",
    "$$\n",
    "p(dog) =p(dog\\ |\\ mammal) * p(mammal\\ |\\ animal) * p(animal\\ |\\ physical object)\n",
    "$$\n",
    "\n",
    "In summary, the flexibility given by the hierarchical model allows the classifier to produce the most granular classification possible given the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages to install\n",
    "- h5py\n",
    "- pillow\n",
    "- pydot\n",
    "- graphviz\n",
    "\n",
    "Download the yolo.h5 weights from\n",
    "\n",
    "https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5   \n",
    "\n",
    "Note that this file is 237 MB. You will need to have adequate disk space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2019, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
