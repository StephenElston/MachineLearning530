{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Object Recognition\n",
    "\n",
    "## Overview\n",
    "\n",
    "In other lessons we have primarily dealt with classification problems. We have worked with images with a single object which we classify into its category.    \n",
    "\n",
    "**Object recognition** is a more difficult task. The process allows us to deal with complex images containing multiple objects. There are two major sets in this process:    \n",
    "1. **Detect** the presence of each of multiple objects in an image. In other lessons, we have dealt with object classification, where there is a single object of interest in the image. Having an unknown number of objects in the image introduces considerable difficulty in the problem. Another difficulty is the trade-off between correctly detecting the objects of interest and falsely detecting spurious objects in the background.  \n",
    "2. Compute a minimum size **bounding box** around each object. \n",
    "3. The object detected in each of the bounding box is **classified**. \n",
    "\n",
    "Our goal is to give you an understanding of how modern object recognition algorithms work. Given the research momentum in this area it is entirely likely that there will be better performing algorithms by the time you read this. \n",
    "\n",
    "The object recognition problem has been around for quite some time. Historically, object recognition was performed using hand engineered features. By 2012 the **mean average precision** or **mAP** of object recognition had stagnated at around 40%. Only since 2013 there has been a surge of both interest and performance of object recognition with the introduction of deep learning methods. Since that time, the mAP achieved has continued to increase and is now routinely over 80%.  These algorithms no longer require the cumbersome process of hand engineering features. Some key papers in the history of deep object recognition are:\n",
    "\n",
    "1. In 2013 [Erhan et. al.](https://arxiv.org/pdf/1312.2249.pdf) published Scalable Object Detection using Deep Neural Networks, which introduced the R-CNN algorithm the first widely accepted deep learning object detection algorithm. R-CNN demonstrated a significant improvement in object recognition accuracy. However, this algorithm proved to be too slow for real-time video processing. \n",
    "2. The Fast R-CNN algorithm was introduced by [Girshick](https://arxiv.org/pdf/1504.08083.pdf) Fast R-CNN simplified the required computations but still struggled with real-time video.  \n",
    "3. Further improvements by [Ren et. al.](https://arxiv.org/pdf/1506.01497.pdf) lead to the Faster R-CNN algorithm. However, the computational complexity of this algorithm was still rather high.  \n",
    "4. The Mask R-CNN algorithm was proposed by [He, et. al. in 2018](https://arxiv.org/pdf/1703.06870.pdf) The Mask R-CNN algorithm exhibits significantly improved object detection accuracy. This is particularly the case where there are large numbers of objects, such as flock of birds or a crowd of people. While not efficient enough for real-time video, Mask R-CNN should be considered if accuracy in complex scenes is required. \n",
    "\n",
    "All of these algorithms share a similar architecture. This architecture is in the form of a pipeline with process steps computed sequentially. The key steps in these pipelines generally include: \n",
    "\n",
    "1. **Convolutional Neural Network:** The CNN creates features which which are used to detect and then classify objects in the image.   \n",
    "2. **Candidate bounding boxes:** Candidate bounding boxes are generated. Multiple candidate bounding boxes cover each region. \n",
    "3. **Filtering of bounding boxes:** The probability of an object being in each bounding box is computed and low probability boxes are filtered. \n",
    "4. **Minimal bounding boxes:** The size of the bounding boxes must be adjusted to best fit the objects detected. \n",
    "5. **Classification:** The objects in the bounding box are classified. \n",
    "\n",
    "The result is that training and performing inference with these complex pipelines has significant computational complexity. Further, these pipelines can be difficult to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The YOLO Algorithms\n",
    "\n",
    "In this lesson we will not focus on the R-CNN family of algorithms. Instead, we will focus on the You Only Look Once or YOLO algorithms. YOLO was first introduced in by [Redmon et. al.](https://arxiv.org/pdf/1506.02640v5.pdf) in 2016. \n",
    "\n",
    "The YOLO algorithms exhibit slightly less accuracy than say Fast R-CNN and its derivatives. However, YOLO is considerably more efficient and can truly be applied to real-time video. At the time of this writing the YOLO family of algorithms are generally considered to be the state of the art in real-time object recognition. \n",
    "\n",
    "YOLO does have some limitations. For example, it is known that YOLO algorithms do not perform well when faced with a large number of small objects, such as a flock of birds. Mask-RNN algorithms exhibit considerably better performance in these cases, but at a cost in speed.      \n",
    "\n",
    "There are two significant breakthroughs with YOLO:\n",
    "1. The recognition that object detection and classification can be performed in one pass. Hence the name You Only Look Once. All of the R-CNN algorithms required multiple passes to perform object recognition. In some cases several thousand passes are required, which was computationally intensive. \n",
    "2. YOLO is trained as a single model. R-CNN required the training of multiple models, for recognition, bounding box,and classification. To be fair, Fast R-CNN reduced the number of models to be trained to two.\n",
    "\n",
    "Since the original YOLO paper there have been two improved versions of the YOLO algorithms have been published. The latest information on YOLO can be found on the [YOLO web pages](https://pjreddie.com/darknet/yolo/), which is part of the Darknet repository. The [YOLO9000 algorithm](https://arxiv.org/pdf/1612.08242.pdf), which is often referred to as YOLO v2, is a significant improvement over the original YOLO algorithm. Lately, some incremental improvements have been published as [YOLO v3](https://pjreddie.com/media/files/papers/YOLOv3.pdf).\n",
    "\n",
    "********\n",
    "**Note:** As has already been stated, deep object detection is an active area of research. Another computationally efficient algorithms [SSD:  Single Shot MultiBox Detector was proposed by Liu et. al. in 2016](https://arxiv.org/pdf/1512.02325.pdf). \n",
    "********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU\n",
    "\n",
    "Object detection requires a metric to determine how well the computed bounding box fits the object. The metric we use is known as Intersection over Union or IoU. This metrics is used to compare human marked bounding boxes with the ones computed by the algorithm. \n",
    "\n",
    "The concept of IoU is illustrated in the figure below:\n",
    "\n",
    "<img src=\"img/IoU.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>**Intersection and union of bounding boxes**</center>\n",
    "\n",
    "Given the two bounding boxes the IoU is easily computed as:\n",
    "\n",
    "$$IoU = \\frac{Area\\ of\\ intersection}{Area\\ of\\ union}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding box parameterization\n",
    "\n",
    "How do we perform object detection and compute a bounding box using a regression model? First, we need a parameterization for the bounding box suitable for regression. There are 4 spacial parameters along with the probability that an object is in the box. For each bounding box the parameterization is:   \n",
    "\n",
    "$$\n",
    "b_x = \\sigma(t_x) + c_x\\\\\n",
    "b_y = \\sigma(t_y) + c_y\\\\\n",
    "b_w = p_w e^{t_w}\\\\\n",
    "b_h = p_h e^{t_h}\\\\\n",
    "p_0 = Pr(object) * IoU(b,\\ object) = \\sigma(t_0)\n",
    "$$\n",
    "\n",
    "These relationships are illustrated in the figure below. The \n",
    "\n",
    "<img src=\"img/BoundingBox1.jpg\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center>**Proposal bounding box in blue and optimized bounding box in red**</center>\n",
    "\n",
    "The above relationships include priors, often known as proposals, for the parameters of the bounding box. These are the initial values of for the regression and are illustrated in the figure above in blue.      \n",
    "$c_x$ is the prior of the x coordinate of the bounding box,   \n",
    "$c_y$ is the prior of the y coordinate of the bounding box,     \n",
    "$b_w$ is the prior of the width of the bounding box, and     \n",
    "$b_h$ is the prior of the height of the bounding box.    \n",
    "\n",
    "In the above figure the following notation is used to parameterize the best fit bounding box, which is shown in red: \n",
    "$b_x$ is the x coordinate of the center of the bounding box,   \n",
    "$b_y$ is the y coordinate of the center of the bounding box,  \n",
    "$b_w$ is the width of the bounding box,   \n",
    "$b_h$ is the height of the bounding box, and   \n",
    "$p_0$ is the probability of an object in the box.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection by regression\n",
    "\n",
    "Now that we have a parameterization of the object detection problem let's formulate the regression model. Using this parameterization, the best fit bounding box is computed using a regression model. Additionally, we need to classify the object in the bounding box. Our goal is to compute the smallest bounding box that encloses an object along with the probability there is an object in the box and the class of the object. By using a regression model to estimate all of the parameters \n",
    "\n",
    "Training the regression model uses labeled or known cases. The known cases typically have human marked bounding boxes around known objects. The presence and class of the object are also labeled.  \n",
    "\n",
    "To illustrate the concepts we will start with just a single bounding box with three possible object classes. The vector of parameters to be estimated in this regression problem can be written as follows:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = \n",
    "\\begin{bmatrix}b_x \\\\ b_y \\\\ b_w \\\\ b_h \\\\ p_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The first 5 parameters in this vector have already been discussed. The values of $\\{ c_1, c_2, c_3 \\}$ are binary, and indicate which class the object falls in. Typically, the regression model is trained by minimizing the RMS error with the labeled values of the parameters. \n",
    "\n",
    "Next, let's expand the simple single bounding box model to a multiple bounding boxes. An example of multiple bounding box proposals sharing a single center, or **anchor**, is shown in the figure below. An object could have various shapes, resulting in a higher probability for one of the bounding boxes enclosing the object than the others.     \n",
    "\n",
    "<img src=\"img/MultiBox.jpg\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center>**Bounding boxes with common center or anchor point**</center>\n",
    "\n",
    "The above figure shows multiple bounding boxes with a common anchor box. On a real image there will be many possible anchors each with multiple bounding box proposals. In this case, we show 4 boxes to keep things simple. But, keep in mind that Yolo typically uses 5 boxes per anchor.  We can generalize the vector of model parameters of the regression model as shown here. The parameters are repeated in the vector for each bounding box.     \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}} = \n",
    "\\begin{bmatrix}b_x \\\\ b_y \\\\ b_w \\\\ b_h \\\\ p_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\\\ \\vdots \\\\\n",
    "b_x \\\\ b_y \\\\ b_w \\\\ b_h \\\\ p_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{bmatrix} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-scale feature generation with CNNs\n",
    "\n",
    "Good object recognition requires good feature generation. As has already been mentioned, the application of recurrent neural networks for feature generation in object recognition was a major breakthrough.   \n",
    "\n",
    "A recent advance in object recognition is the application of multi-scale features using the hierarchy of RNNs. This method was first introduced by [Lin et all](https://arxiv.org/pdf/1612.03144.pdf) in 2017. \n",
    "\n",
    "We have discussed RNNs in another lesson. In that lesson we saw how the scale of the features changes  The heirarchy of RNN layers creates features features \n",
    "\n",
    "<img src=\"img/multiscale.jpg\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center>**Multi-scale feature pyramid**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-max Suppression\n",
    "\n",
    "The result of the regression produces the optimal dimensions of the bounding boxes, the probabilities and the classification of the object. There are multiple bounding boxes covering the same area of the image. Therefore, multiple bounding boxes will overlap with each object detected. We need a way to find a single best bounding box for each object. The **non-max suppression algorithm** is just such an algorithm. \n",
    "\n",
    "The non-max suppression algorithm proceeds in two steps:\n",
    "1. The probabilities of objects in each of the boxes is computed.\n",
    "2. Boxes with probabilities below some threshold, say 0.6, are eliminated from consideration.\n",
    "3. Select the remaining box with the highest probability. \n",
    "4. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heriarchical Feature Extraction\n",
    "\n",
    "\n",
    "[Introduction to WordNet: An On-line Lexical Database by Miller, et. al., 1993](http://wordnetcode.princeton.edu/5papers.pdf)\n",
    "\n",
    "<img src=\"img/flatlabels.jpg\" alt=\"Drawing\" style=\"width:700px; height:125px\"/>\n",
    "<center>**Flat feature labels**</center>\n",
    "\n",
    "<img src=\"img/hierarchicallabels.jpg\" alt=\"Drawing\" style=\"width:700px; height:250px\"/>\n",
    "<center>**Hierarchical feature labels**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages to install\n",
    "- h5py\n",
    "- pillow\n",
    "- pydot\n",
    "- graphviz\n",
    "\n",
    "Download the yolo.h5 weights from\n",
    "\n",
    "https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5   \n",
    "\n",
    "Note that this file is 237 MB. You will need to have adequate disk space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection Over Union \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
