{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dynamic Programming\n",
    "\n",
    "## CSCI E-82A\n",
    "## Stephen Elston\n",
    "\n",
    "In the previous lesson we explored the concepts of **Markov decision processes (MDP)** and **Markov reward processes (MRP)**. Now we will extend these concepts and apply them to finding **optimal solutions** for such systems. By an optimal solution, we mean a solution which produces **greater reward** than any other solution. \n",
    "\n",
    "To understand how we can find optimal solutions we must introduce some new concepts:\n",
    "1. An **action** causes a state transition. This transition may be to the same state. \n",
    "2. A **policy** specifies the **action** given the state. In other words, the policy defines the actions to be taken by the agent, given the state. \n",
    "3. An **optimal policy** produces the greatest reward possible given the initial state of the system.\n",
    "4. A **plan** is the sequence of actions leading to an optimal result given the reward structure. \n",
    "\n",
    "In this and subsequent lessons, we will explore some powerful methods for finding optimal policies. Broadly, these methods are known as **dynamic programming** and **reinforcement learning**. In this lesson we will focus on the representation and learning methods for dynamic programming. Dynamic programming algorithms can be the basis of effective and flexible intelligent agents.\n",
    "\n",
    "<img src=\"img/DPAgent.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Dynamic Programming Agent and Environment** </center>  \n",
    "\n",
    "\n",
    "**Suggested readings** for dynamic programming are:\n",
    "- Sutton and Barto, second edition, Sections 3.7, 3.8, and Chapter 4 or,   \n",
    "- Russell and Norvig, third edition, Sections 17.1, 17.2 or, \n",
    "- Kochenderfer, Sections 4.1, 4.2, 4.3, 4.5, 4.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Dynamic Programming?\n",
    "\n",
    "What is **dynamic programming**? In the most general terms, dynamic programming is a **optimal planning method**. Planning methods enable an intelligent agent to gain improved autonomy though a sequence of optimal actions to achieve a **goal** given a model of the environment. The requirement of a model is a significant requirement for planning with dynamic programming. \n",
    "\n",
    "Dynamic programming was developed in the 1950's by mathematician [**Richard Bellman**](https://en.wikipedia.org/wiki/Richard_E._Bellman). By *programming* Bellman meant a computer algorithm which computes a a plan of actions to optimize the **utility** or **total reward** for the states visited in a Markov process. By *dynamic*, Bellman meant the algorithm solves the problem recursively by operating on smaller and simpler sub-problems. \n",
    "\n",
    "Like dynamic programming, **reinforcement learning** is class of optimization algorithms using a sequence of actions for a system represented by a Markov processes. Reinforcement learning algorithms are considered **model free**. None the less, understanding dynamic programming is good path to understanding reinforcement learning. \n",
    "\n",
    "******************************************\n",
    "**Note:** If you are familiar with machine learning, you may recognize Bellman as the person who coined the phrase *the curse of dimensionality*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Given a Markov random process, a reward function, and a policy of actions, how can we evaluate a policy? We can perform **policy evaluation** in two ways, using a value function or using an action value function.  \n",
    "\n",
    "### Policy Evaluation with Value Functions\n",
    "\n",
    "First, we can perform **policy evaluation** with a **value function**. The value function is the expected value of the **gain** achieved by following a policy, $\\pi(a|s)$, given the current state. We can express the state value function as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{\\pi}(s) &= \\mathbb{E}_{\\pi} [ G_t\\ |\\ S_t = s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma G_{t+1}\\ |\\ S_t = s] \\\\\n",
    "&= \\mathbb{E}_{\\pi} [R_{t+1} + \\gamma v_{\\pi} (S_{t+1})\\ |\\ S_t = s] \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s'} \\sum_{r} p(s',r\\ | s,a) \\big[ r + \\gamma v_{\\pi}(s') \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r\\ | s,a) \\big[ r + \\gamma v_{\\pi}(s') \\big],\\ \\forall a \n",
    "\\end{align}\n",
    "$$   \n",
    "Where,    \n",
    "$\\mathbb{E}_{\\pi} [] =$ Expectation given policy $\\pi$,   \n",
    "$v_{\\pi}(s) =$ value of state, s, given policy $\\pi$,   \n",
    "$G_t =$ return at step $t$,    \n",
    "$S_t =$ state at step $t$,    \n",
    "$R_t =$ reward at step $t$,    \n",
    "$a = $ an action by the agent,   \n",
    "$\\pi(a|s) =$ the policy specifying the probability of action, $a$, given state, $s$,  \n",
    "$p(s',r\\ | s,a) =$ probability of successor state, $s'$, and reward, $r$, given state, $s$, and action $a$,  \n",
    "$\\big[ r + \\gamma v_{\\pi}(s') \\big] =$ the bootstrapped state value,  \n",
    "$\\gamma =$ discount rate. \n",
    "\n",
    "\n",
    "The above relations are known as the **Bellman value equations**. This relationship tells us how to compute the value of being in a particular state, $s$. There is one such equation for each state, $s \\in \\xi$, of the Markov process.\n",
    "\n",
    "In principle, we could find the state values by solving the $\\xi \\in R^n$ simultaneous linear Bellman state value equations. However, this approach has computational complexity $O(n^3)$. \n",
    "\n",
    "Examine the last line of the above relations and notice it can be viewed as a **recursion**. This leads us to an iterative solutions. We can start with an initial set of state values (e.g, 0) and iterate the equation through $T$ time steps, until meeting a **convergence criteria**. The convergence criteria is a measure in the change of the state values from one iteration to the next. \n",
    "\n",
    "We can solve the Bellman equations using a recursion relationship. In this recussion, the state value equations are iterated though the values of the successor states. The estimate of state value is improved at each step of the recussion using the estimates of successor state values. We call such an algorithm a **Bootstrap method**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation with Action Value Functions\n",
    "\n",
    "As an alternative to state values, we can use **action values**:\n",
    "\n",
    "$$q_{\\pi}(s,\\ a) = \\mathbb{E}_{\\pi} [ G_t\\ |\\ S_t = s,\\ A_t = a]$$\n",
    "\n",
    "The action values can be computed using the **Bellman state action function**:\n",
    "\n",
    "$$\\begin{align}\n",
    "q_\\pi(s,a) &= \\mathbb{E_\\pi} \\big[G_{t}\\ \\big|\\ S_t = s, A_t = a \\big]  \\\\\n",
    "&= \\mathbb{E_\\pi} \\Big[ \\sum_{k=0}^\\infty \\gamma^k\\ R_{t+k+1} \\big|\\ S_t = s, A_t = a \\Big]  \\\\\n",
    "&= \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma\\ q_\\pi(S_{t+1},a') \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "Where,\n",
    "$q_\\pi(s,a) =$ is the action value, which is the value of action, $a$ from state $s$ following policy $\\pi$,   \n",
    "$A_t = $ the action taken at step $t$,  \n",
    "$a' = $ the action taken from the successor state, $s'$,  \n",
    "and other notation is the same as discussed for the state value function. \n",
    "\n",
    "\n",
    "Examining the last line of the above relationship shows that this is a recursion relation. Thus, we can bootstrap with the action value function to iteratively find the action values of the Markov process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Value for Grid World Example\n",
    "\n",
    "Let's try an example of computing the state values of a Markov process. In this example, we will work with a **representation** of the Markov process model. We will apply the state value function approach. In a later example we will use **learning** to improve policy. \n",
    "\n",
    "**Navigation** to a goal is a significant problem in robotics. Real-world navigation is rather complex. Therefore, in this example we will use a simple analog called a **grid world**. The grid world for this problem is shown below. \n",
    "\n",
    "<img src=\"img/GridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **A 4x4 Grid World with Terminal State** </center>\n",
    "\n",
    "The grid world consists of a 4x4 set of positions the robot can occupy. Each position is considered a state. The agent must navigate navigate the robot to state 0, the goal, in the minimum number of steps. We will explore methods to find policies which achieve this maximum reward. \n",
    "\n",
    "Grid position 0 is the goal or **terminal state**. There are no possible transitions out of this states. The presence of a terminal state makes this an **episodic Markov random process**. In each episode the robot can start in any other random position, $\\{ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \\}$, and the episode terminates when the robot enters the terminal position (state 0).  \n",
    "\n",
    "In each state, there are four possible actions the robot can take:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "We encode, or represent, these possibilities in an Python dictionary as shown in the code block below. We use a dictionary of dictionaries to perform the lookup for each possible state. The keys of the outer dictionary are the identifiers (numbers) of the states. The keys of the inner dictionary are the possible actions and the values are the **successor state**, $s'$, for that transition.  \n",
    "\n",
    "Examine this dictionary. Notice that there are no allowed transitions out of the terminal state. Also, any transition that tries to take the robot off the grid, leaves the state unchanged. \n",
    "\n",
    "> **Note:** For large scale solutions a more efficient lookup is used for the model. For now, we are using the dictionary of dictionaries since it is easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors = {0:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':9, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':10, 'd':14, 'l':13, 'r':15},\n",
    "          15:{'u':11, 'd':15, 'l':14, 'r':15}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the initial transition probabilities for the Markov process. We set the probabilities for each transition as a **uniform distribution** leading to random actions by the robot. As there are 4 possible transitions from each state, this means all transition probabilities are 0.25. In other words, this is a random policy which does not favor any particular plan. \n",
    "\n",
    "The initial uniform transition probabilities are encoded using a dictionary of dictionaries. The organization of this data structure is identical to the foregoing data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_policy = {0:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- 10 for entering position 0. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. The penelty forces the agent to find a shortest distance path for the robot.  \n",
    "\n",
    "We encode this reward in the same type of dictionary structure used for the foregoing structures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "          1:{'u':-1, 'd':-0.1, 'l':10.0, 'r':-0.1},\n",
    "          2:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          4:{'u':10.0, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          6:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          7:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          8:{'u':-0.1, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          11:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          12:{'u':-0.1, 'd':-1.0, 'l':-1.0, 'r':-0.1},\n",
    "          13:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          15:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we are using a rather poor policy, which is just a random walk around the grid. Still, we can still measure the value of this policy. \n",
    "\n",
    "The function in the code below iterates over the Bellman value function to find the values of each state. The iteration continues until the convergence criteria is meet.  \n",
    "\n",
    "> **Note:** The code in this example takes advantage of the fact that there is only one possible successor state for each action. This means there is no need to sum over successor states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.92461824, -3.93184927, -6.48887002],\n",
       "       [ 0.92461824, -2.10995108, -4.95119803, -6.86735717],\n",
       "       [-3.93184927, -4.95119803, -6.5102566 , -7.87700873],\n",
       "       [-6.48887002, -6.86735717, -7.87700873, -8.96905736]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_state_value(neighbors, policy, reward, gamma = 1.0, theta = 0.01, display = False):\n",
    "    '''Function for policy evaluation  \n",
    "    '''\n",
    "    delta = theta\n",
    "    values = np.zeros(len(policy)) # Initialize the value array\n",
    "    while(delta >= theta):\n",
    "        v = np.copy(values) ## save the values for computing the difference later\n",
    "        for s in policy.keys():\n",
    "            temp_values = 0.0 ## Initial the sum of values for this state\n",
    "            for action in rewards[s].keys():\n",
    "                s_prime = neighbors[s][action]\n",
    "                temp_values = temp_values + policy[s][action] * (reward[s][action] + gamma * values[s_prime])\n",
    "            values[s] = temp_values\n",
    "            \n",
    "        ## Compute the difference metric to see if convergence has been reached.    \n",
    "        diffs = np.sum(np.abs(np.subtract(v, values)))\n",
    "        delta = min([delta, diffs])\n",
    "        if(display): \n",
    "            print('difference metric = ' + str(diffs))\n",
    "            print(values.reshape(4,4))\n",
    "    return values\n",
    "\n",
    "compute_state_value(neighbors, initial_policy, rewards, theta = 0.1, display = False).reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array displayed above shows the value of being in a state (grid position) given the policy. You can see that the closer the position is to the goal, the higher the value. Intuitively, this result makes sense.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Dynamic Programming and Optimal Policy\n",
    "\n",
    "We now have a method to evaluate a policy for a Markov process. However, the current random policy is rather poor. Fortunately, the Bellman equations lead to several possible dynamic programming algorithms for **policy improvement**. Policy improvement is equivalent to learning a better policy. \n",
    "\n",
    "Before we get too far here, we should define what exactly mean by policy improvement. Intuitively, an improved policy should yield greater total utility than a previous policy. More specifically an improved policy $\\pi'$ has the following relationship with an initial policy, $\\pi$:\n",
    "\n",
    "$$v_{\\pi'}(s) \\ge v_\\pi(s)$$\n",
    "\n",
    "We can also say that an optimal policy $\\pi_*$ must conform to the following:\n",
    "\n",
    "$$v_{*}(s) \\ge v_\\pi(s)\\ \\forall \\pi$$\n",
    "\n",
    "In words, the optimal policy has a value greater than or equal to all other possible policies. Notice there is **no guarantee of uniqueness**. There can be multiple optimal policies.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman optimal state-value equations\n",
    "\n",
    "The key idea of dynamic programming is expressed by the **Bellman optimality equations**. There are two ways we can express this optimal relationship. We can find a relationship which is a solution to the **Bellman optimal state action equations**, denoted $q_{*}(s,a)$. Or, we can find a solution to the **Bellman optimal state value equations**, $v_{*}(s)$, shown here:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_{*}(s) &= \\underset{a \\in A(s)}{max}\\ q_{\\pi^*} (s,a) \\\\\n",
    "&= \\underset{a}{max} \\mathbb{E}_{\\pi^*} [ G_{t} |\\ S_t = s, A_t = a] \\\\\n",
    "&= \\underset{a}{max} \\mathbb{E}_{\\pi^*} [R_{t+1} + \\gamma G_{t+1} |\\ S_t = s, A_t = a] \\\\\n",
    "&= \\underset{a}{max} \\mathbb{E} [ G_{t+1} + \\gamma v_*(S_{t+1})\\ |\\ S_t = s, A_t = a] \\\\\n",
    "&= \\underset{a}{max} \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma v_{*}(s') \\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Given the $max_a$ operation these equations are no longer linear. But as with the state value functions, the optimal state value equations are naturally recursive and amenable to iterative solutions.\n",
    "\n",
    "We can gain some insight into the iterative process for the Bellman optimal state value equations by studying their **backup diagram**. A backup diagram is read from top to bottom. States are shown as open circles and actions by filled circles. The backup diagram for the Bellman optimal state value iteration is shown below.\n",
    "\n",
    "<img src=\"img/ValueBackup.JPG\" alt=\"Drawing\" style=\"width:250px; height:200px\"/>\n",
    "<center> **Backup diagram of Bellman Optimal State Value Function** </center>\n",
    "\n",
    "Let's follow this backup diagram from top to bottom.\n",
    "- The Markov process starts in some initial state $s$. \n",
    "- We take the action $a$ that maximizes the state value.\n",
    "- This leads to successor states, $s'$ with reward $r$.\n",
    "\n",
    "Why do we call the above relationship a backup? Each step the optimal action is estimated by only looking one state ahead. As the itteration continues we backup into better estimates of the state values and hence improved polices.   \n",
    "\n",
    "Notice that this algorithm, like all dynamic programming methods, requires us to evaluate the value of all possible actions for a given state. You can see this from the way the max sweep is shown in the above diagram. We say that dynamic programming requires **full backups**. This fact puts a lower bound on the computational complexity of dynamic programming methods.   \n",
    "\n",
    "The requirement to perform full backups lead Richard Bellman to declare this as the *curse of dimensionality*. That is as the number of states (dimensions of the problem) grows so goes the computational complexity of dynamic programming. None the less, dynamic programming is a surprisingly scalable method. We have seen how DP is much more efficient that direct solution methods. Further, DP algorithms have significantly lower complexity than the equivalent graph search algorithms or linear programming methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman optimal action-value equations\n",
    "\n",
    "Now, we will investigate using the Bellman **optimal state action equations** for policy improvement. In this context, the policy improvement relations are expressed as:\n",
    "\n",
    "$$q_{*}(s,\\ a) \\ge q_{\\pi}(s,\\ a)\\ \\forall\\ \\pi$$\n",
    "\n",
    "In words, the optimal state action policy $\\pi_{*}(s)$ gives a value greater than or equal to all other state action policies. Once again, there is no guarantee of uniqueness. \n",
    "\n",
    "The Bellman optimal state action equations are expressed:\n",
    "\n",
    "$$\\begin{align}\n",
    "q_{*}(s,a) &= \\underset{\\pi}{max}\\ q(s,a)\\\\\n",
    "&= \\mathbb{E} \\big[R_{t+1} + \\gamma \\underset{a}{max}\\ q_{*}(S_{t+1},a')\\ \n",
    "\\big|\\ S_t = s, A_t = a \\big]  \\\\\n",
    "&= \\underset{a}{max} \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma\\ q_{*}(S_{t+1},a) \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "Where, $q_{*}(s,a)$ is the policy giving action, $a$, from a state, $s$ given **optimal policy**, $\\pi_*$. As with the Bellman state action value equations, there is one such equation for each action value tuple, $(a,s)$.\n",
    "\n",
    "\n",
    "We can gain some understanding of how the recursion works by looking at the **backup diagram** shown below.\n",
    "\n",
    "<img src=\"img/ActionValueBackup.JPG\" alt=\"Drawing\" style=\"width:250px; height:200px\"/>\n",
    "<center> **Backups diagram of Bellman Optimal Action Value Function** </center>\n",
    "\n",
    "\n",
    "Starting at the top of the diagram:\n",
    "- The Markov process starts with a state-action tuple, $(s,a)$.\n",
    "- This state-action leads to successor states, $s'$ with reward $r$.\n",
    "- The successor action, $a'$, with the maximum expect value is selected. \n",
    "\n",
    "As with policy iteration, notice that value iteration requires a full backup at each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration for Grid World\n",
    "\n",
    "Let's try the **policy iteration** algorithm on the grid world example. The core idea of policy iteration is to alternately evaluate and then improve the policy, $\\pi$. Each iteration yields a subsequent improved policy, $\\pi'$. The process is illustrated by the relationship shown below:\n",
    "\n",
    "$$\\pi_0 \\overset{E}{\\rightarrow} v_{\\pi_0} \\overset{I}{\\rightarrow} \\pi_1 \n",
    "\\overset{E}{\\rightarrow} v_{\\pi_1} \\overset{I}{\\rightarrow} \\pi_2\n",
    "\\overset{E}{\\rightarrow} \\cdots \n",
    "\\overset{I}{\\rightarrow} \\pi_* \\overset{E}{\\rightarrow} v_{*} $$\n",
    "\n",
    "As the policy iteration algorithm is iterative, the process can also be viewed as shown in the figure below:\n",
    "\n",
    "<img src=\"img/GPI.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center>**Illustration showing steps of policy iteration algorithm**</center>\n",
    "\n",
    "The function in the cell below performs policy iteration in two major steps:\n",
    "1. The state values are computed by call the `compute_state_value` function previously defined.\n",
    "2. The policy with the maximum value is found for all possible actions given the state values.\n",
    "\n",
    "Since the grid world problem is simple, only one iteration is required. More complex problems would require multiple iterations. \n",
    "\n",
    "The code in the cell below implements this algorithm and applies it to the grid world. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 2: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 3: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 4: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 5: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 6: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 7: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 8: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 9: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 10: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 11: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 12: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 13: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 14: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 15: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def policy_iteration(policy, neighbors, reward, gamma = 1.0, theta = 0.1, goal = 0, output = False):\n",
    "    ## As a first step compute the state values \n",
    "    state_values = compute_state_value(neighbors, policy, reward, gamma = gamma, theta = theta)\n",
    "    \n",
    "    ## Now, need to find the deterministic policy that \n",
    "    ## makes max value state transitions.\n",
    "    for s in policy.keys(): # iterate over all states\n",
    "        ## Find the indicies of maximum state transition values\n",
    "        ## There are two cases. \n",
    "        ## First, the special case of a state adjacent to the goal\n",
    "        ## In this case need to ensure the only possible transition is to the goal.\n",
    "        ## Start by creating a list of the adjacent states.\n",
    "        possible_s_prime = [val for val in neighbors[s].values()]\n",
    "        ## Check if goal is adjacent, but state is not the goal.\n",
    "        if(goal in possible_s_prime and s != goal):\n",
    "            temp_values = []\n",
    "            for s_prime in policy[s].keys(): # Iterate over adjacent states\n",
    "                if(neighbors[s][s_prime] == goal):  ## account for the special case adjacent to goal\n",
    "                    temp_values.append(reward[s][s_prime])\n",
    "                else: temp_values.append(0.0) ## Other transisions have 0 value.\n",
    "        ## The other case is rather easy requires a lookup of the value of the \n",
    "        ## adjacent state and handled with a list comprehension.             \n",
    "        else: temp_values = [state_values[s_prime] for s_prime in neighbors[s].values()] \n",
    "                \n",
    "        ## Find the index for the state transistions with the largest values \n",
    "        ## May be more than one. \n",
    "        max_index = np.where(np.array(temp_values) == max(temp_values))[0]  \n",
    "        prob_for_policy = 1.0/float(len(max_index)) ## Probabilities of transition\n",
    "        \n",
    "        ##### ADD CODE TO FIND THE INDEX OF EACH MAXIMUM VALUE AND FIND  ##############\n",
    "        ##### THE PROBABIITY OF THE STATE TRANSITION FOR THE NUMBER OF   ##############\n",
    "        ##### MAXIMUM FOUND. THERE MAY BE MORE THAN ONE MAXIMUM          ##############\n",
    "        for i, key in enumerate(policy[s]): ## Update policy\n",
    "            if(i in max_index): policy[s][key] = prob_for_policy\n",
    "            else: policy[s][key] = 0.0       \n",
    "    return policy\n",
    "\n",
    "\n",
    "policy_iteration(initial_policy, neighbors, rewards, gamma = 1.0, output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we interpret this optimal plan? The plan specifies the optimal state transitions to travel from any state to the goal. For example, the diagram below shows the optimal paths from State 15 to the goal. Notice that these paths are not unique in this plan. \n",
    "\n",
    "<img src=\"img/OptimalPath.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Optimal Paths From State 15 to Goal** </center>\n",
    "\n",
    "Any of the 4 possible alternatives illustrated above are optimal:\n",
    "\n",
    "- 15 -> 11 -> 7 -> 6 -> 5 -> 1 -> 0\n",
    "- 15 -> 11 -> 7 -> 6 -> 5 -> 4 -> 0\n",
    "- 15 -> 14 -> 13 -> 9 -> 5 -> 1 -> 0\n",
    "- 15 -> 14 -> 13 -> 9 -> 5 -> 4 -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration for Grid World\n",
    "\n",
    "Let's try the **value iteration algorithm** on the grid world example. Using the Bellman optimal state-value equations, we can express the recursion relationship of the value iteration algorithm:\n",
    "\n",
    "$$\\begin{align}\n",
    "v_{k+1}(s) &= \\underset{a}{max} \\mathbb{E} \\big[R_{t+1} + \\gamma v_{k}(S_{t+1})\\ \n",
    "\\big|\\ S_t = s, A_t = a \\big]  \\\\\n",
    "&= \\underset{a}{max} \\sum_{s',r} p(s',r\\ |\\ s,a) \\big[ r + \\gamma\\ v_{k}(s') \\big]\n",
    "\\end{align}$$\n",
    "\n",
    "The function in the cell below performs value iteration algorithm by the following steps:\n",
    "1. For possible state actions, the maximum state value is found.\n",
    "3. The change in state-value is measured to find if the algorithm has converged. If not steps 1 is repeated. \n",
    "3. Given the resulting optimal state actions, the optimal policy $\\pi_*$ is found by taking the maximum over possible actions for each successor states. This step must account for a few special cases outlined in the code comments below. \n",
    "\n",
    "The code in the cell below implements this algorithm and applies it to the grid world. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 2: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 3: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 4: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 5: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 6: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 7: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 8: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 9: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 10: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 11: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 12: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 13: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 14: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0},\n",
       " 15: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration(policy, neighbors, reward, goal, gamma = 1.0, theta = 0.1, output = False):\n",
    "    delta = theta\n",
    "    v = np.zeros(len(neighbors))\n",
    "    state_values = np.zeros(len(neighbors))\n",
    "    while(delta >= theta):\n",
    "        for s in neighbors.keys(): # iteratve over all states\n",
    "            temp_values = []\n",
    "            ## Find the values for all possible actions in the state.\n",
    "#            for action in rewards[s].keys():\n",
    "            for action in neighbors[s].keys():    \n",
    "                s_prime = neighbors[s][action]\n",
    "                temp_values.append((reward[s][action] + gamma * state_values[s_prime]))\n",
    "            \n",
    "            ## Find the max value and update the value for the state\n",
    "            #### ADD THE CODE TO FIND THE MAXIMUM OF THE temp_values #######\n",
    "            #### AND UPDATE THE STATE VALUE ARRAY                    #######\n",
    "            state_values[s] = max(temp_values)\n",
    "        ## Determine if convergence is achieved\n",
    "        diff = np.sum(np.abs(np.subtract(v, state_values)))\n",
    "        delta = min([delta, np.sum(np.abs(np.subtract(v, state_values)))])\n",
    "        v = np.copy(state_values)\n",
    "        if(output):\n",
    "            print('Difference = ' + str(diff))\n",
    "            print(state_values.reshape(4,4))\n",
    "    \n",
    "    ## Now we need to find the deterministic policy that \n",
    "    ## makes max value state transitions.\n",
    "    for s in policy.keys(): # iterate over all states\n",
    "        ## Find the indicies of maximum state transition values\n",
    "        ## There are two cases. \n",
    "        ## First, the special case of a state adjacent to the goal\n",
    "        ## In this case need to ensure the only possible transition is to the goal.\n",
    "        ## Start by creating a list of the adjacent states.\n",
    "        possible_s_prime = [state for state in neighbors[s].values()]\n",
    "        ## Check if goal is adjacent, but state is not the goal.\n",
    "        if(goal in possible_s_prime and s != goal):\n",
    "            temp_values = []\n",
    "            for a_prime in neighbors[s].keys(): # Iterate over adjacent states\n",
    "                if(neighbors[s][a_prime] == goal):  ## account for the special case adjacent to goal\n",
    "                    temp_values.append(reward[s][a_prime])\n",
    "                else: temp_values.append(0.0) ## Other transisions have 0 value.\n",
    "        ## The other case is rather easy requires a lookup of the value of the \n",
    "        ## adjacent state and handled with a list comprehension.             \n",
    "        else: temp_values = [state_values[s_prime] for s_prime in neighbors[s].values()]\n",
    "                \n",
    "        ## Find the index for the state transistions with the largest values \n",
    "        ## May be more than one. \n",
    "        max_index = np.where(np.array(temp_values) == max(temp_values))[0]  \n",
    "        prob_for_policy = 1.0/float(len(max_index)) ## Probabilities of transition\n",
    "        for i, key in enumerate(policy[s]): ## Update policy\n",
    "            if(i in max_index): policy[s][key] = prob_for_policy\n",
    "            else: policy[s][key] = 0.0       \n",
    "    return policy\n",
    "\n",
    "value_iteration(initial_policy, neighbors, rewards, goal = 0,  output = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting optimal plan is similar to the one developed by policy iteration. There are several solution options. In both cases, the utility of an set of state transitions from the initial state the goal is the same. Thus, both of the solution methods create optimal, but different, results.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, 2019, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
