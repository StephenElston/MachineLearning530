{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Markov Decision Processes\n",
    "\n",
    "## Machine Learning 530\n",
    "### Stephen Elston\n",
    "\n",
    "In this lesson we will introduce **Markov processes**, which are a **representation** of a **memoryless state transition processes**. The diagram below shows the Markov process representation in the intelligent agent, along with the interactions with the environment.   \n",
    "\n",
    "<img src=\"img/AgentEnvironment.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Interaction of agent and environment** </center>\n",
    "\n",
    "**Markov decision processes** which are widely used in planning and optimal decision theory. The closely related **Markov reward process** is necessary for planning and optimal decision methods. We will introduce the Markov reward process here. Markov decision processes are addressed in another lesson. \n",
    "\n",
    "Suggested readings: The following reading is a supplement to the material presented here:\n",
    "\n",
    "Sutton and Barto, second edition, Sections 3.1, 3.2, 3.3, 3.4, 3.5 or\n",
    "Russell and Norvig, third edition, Section 7.1, or\n",
    "Kochenderfer, Section 4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Processes\n",
    "\n",
    "A first order **Markov process** is a process where the probability of a transition between a **finite set of states** only depends on the current state. In other words, first order **Markov processes have no memory of past states**. The current state has all the relevant information on the history of states.   \n",
    "\n",
    "For the transition between as a state $S_t$, at time $t$ ,to the next state $S'_{t+1}$, at time $t+1$, we can express a Markov process mathematically as follows:\n",
    "\n",
    "$$p[S'_{t+1}\\ |\\ S_1, \\ldots, S_t] = p[S'_{t+1}\\ |\\ S_t]$$\n",
    "\n",
    "For a vector of possible states, $S$, we can create a **state transition probability matrix**. This matrix **represents** the probability of a state transition from $S$ to the next state, $S'$ at the next time step:\n",
    "\n",
    "$$\\mathcal{P_{ss'}} = \n",
    "\\begin{bmatrix}\n",
    "    p_{11} & p_{12} & \\dots  & p_{1n} \\\\\n",
    "    p_{21} & p_{22} & \\dots  & p_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    p_{n1} & p_{n2} & \\dots  & p_{nn}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where, $\\mathcal{p}_{ij} =$ probability of transition from state $s_j$ to $s'_i$.   \n",
    "\n",
    "Let's say we have a vector of probabilities of being in one of n possible states, $\\mathcal{S} = (s_1, s_2, \\ldots, s_n)$. Using simple matrix multiplication we can write the relationships for the transition to the next state $\\mathcal{S'}$ as:\n",
    "\n",
    "$$S' = \\mathcal{P_{ss'}} S\\\\\n",
    "or\\\\\n",
    "\\begin{bmatrix}\n",
    "    s_1' \\\\\n",
    "    s_2' \\\\\n",
    "    \\vdots \\\\\n",
    "    s_n'\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "     p_{11} & p_{12} & \\dots  & p_{1n} \\\\\n",
    "    p_{21} & p_{22} & \\dots  & p_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    p_{n1} & p_{n2} & \\dots  & p_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    s_1 \\\\\n",
    "    s_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    s_n\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "A **Markov chain** is a sequence of of states of a Markov process. In other words, if we run a Markov process over a number of time steps the result is a Markov chain. \n",
    "\n",
    "If the transition probability matrix, $ \\mathcal{P_{ss'}}$, does not change over time, we say the Markov chain is **stationary**. Stationary Markov chains have a **convergence property**. If we run the Markov chain for enough time steps, the chain will reach a **steady state**. At steady state the probabilities of being in any state of the Markov process is **unchanged from time step to time step**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Example - Does Steve Need a New Car?\n",
    "\n",
    "Let's try a computational example to test out the foregoing concepts. In this case we will look at the state transitions for the use of an old car vs. a new car. The diagram below shows the states of car ownership and the possible transitions between them.   \n",
    "\n",
    "<img src=\"img/CarStates.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> States and possible transitions of car use </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states and the possible transitions are:\n",
    "1. Old car, can transition to continue driving the old car, a breakdown, or an accident.\n",
    "2. Old car breakdown, can transition to old car or new car.\n",
    "3. Old car accident, transitions to new car.\n",
    "4. New car, can transition to continue driving the new car, a breakdown, or an accident.\n",
    "5. New car breakdown, transitions to new car or to an old car.\n",
    "6. New car accident, transitions to new car.\n",
    "\n",
    "Notice that there are no **terminal states** in this diagram. A terminal state can be entered, but there is no possible transition to another state. An example of a terminal state is the win or loss of a game. The game is over, and there will be no more state transitions. Markov processes with terminal states are said to be **episodic** or **finite** since they run for a finite period of time, after which they must be restarted. Whereas, Markov processes with no terminal state are said to be **infinite**, since in theory they will run for an infinite number of time steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 8-1-1:** Given these transitions, the question is what is the probability that Steve will end up in a new car or keep his old car. You will start by defining a transition probability matrix and testing that the probabilities in the columns add to 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "%matplotlib inline\n",
    "\n",
    "## Define the transition probability matrix below\n",
    "T = \n",
    "\n",
    "\n",
    "print('The transition probability matrix')\n",
    "labels = ['OldCar','OldBreak','NewCar','NewBreak','Accident']\n",
    "print(pd.DataFrame(T, columns = labels, index = labels))\n",
    "\n",
    "print('\\nTest that the columns add to 1')\n",
    "np.sum(T, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now you will do the following to execute a single state transition:   \n",
    "> 1. Define a state vector with the starting state of `OldCar`. In other words, the initial probability is $P(OldCar) = 1.0$.\n",
    "> 2. Compute and print the updated state, $s'$, using [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1:** Notice there are some zero and non-zero state probabilities. Given the state transition matrix, are these zero and non-zero values expected and why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next complete the line of code in the cell below and execute the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_transition(T, s, n = 1):\n",
    "    s_list = np.reshape(s, (s.shape[0],1))\n",
    "    for _ in range(n):\n",
    "        ## Complete the line of code below\n",
    "        s = np.dot(T,s)\n",
    "        s_list = np.concatenate((s_list, np.reshape(s, (s.shape[0],1))), axis = 1)\n",
    "    return \n",
    "\n",
    "states = state_transition(T, initial_state, 50000)\n",
    "pd.Series(states[:,40000], index = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can see the probabilities of being in each state after a large number of transitions.\n",
    "> Now, execute the code in the cell below to make a plot of these states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states(states, labels):\n",
    "    fig = plt.figure(figsize=(6,6)) # define plot area\n",
    "    ax = fig.gca() # define axis \n",
    "    ax.set_xlabel('Number of time steps')\n",
    "    ax.set_ylabel('Probability of being in state')\n",
    "    ax.set_title('Probability of states vs. number of time steps')\n",
    "    for i in range(states.shape[0]):\n",
    "        plt.plot(states[i,:])\n",
    "    ax.legend(labels)    \n",
    "        \n",
    "plot_states(states, labels)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer the following questions with short answers:       \n",
    "> 2. Do these state probabilities appear to have reached a **steady state** and why?   \n",
    "> 3. What do the final probabilities tell you about the chances of driving and old car vs. a new car.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 2. Yes, these state probabilities appear to be in a steady state since the curves are flat, not changing.      \n",
    "> 3. Over time the probability of driving a new car is about 9 times the probability of driving an old car.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Reward Process\n",
    "\n",
    "We can define a **reward function**, $\\mathcal{R}$ as the **expected reward** of **change in utility** in the next time step. Given the expectation over all transitions from the current state $s$ to all possible **successor states** $s'$:\n",
    "\n",
    "$$R_{t+1} = E \\big[ \\mathcal{R}_{s s'}\\ |\\ S_t = s \\big]$$   \n",
    "\n",
    "Where, $\\mathcal{R}_{s s'}$ is the reward for the transition from state, $s$ to a successor state, $s'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of a Markov reward process. The diagram below shows the rewards for the various state transitions in the auto example. Since owning cars has significant costs, all of the rewards are negative. \n",
    "\n",
    "<img src=\"img/CarRewards.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Rewards for state transitions of car use** </center>\n",
    "\n",
    "Keep in mind that just like utility, **reward need not simply follow monetary value**. For example, the reward of the car breaking down must account for the inconvenience of dealing with the repair, or the reward for driving the car must account for intangibles like comfort and safety of the passengers.  \n",
    "\n",
    "What is the relationship between gain and reward in a Markov chain? It is easy to compute gain from rewards, since **rewards are additive**. First, let's consider the gain, $G$, for finite Markov reward process, which reaches a terminal state after $T$ time steps:\n",
    "\n",
    "$$G([s_o, s_1, \\ldots, s_T]) = R(s_o) + R(s_1) + \\ldots + R(s_T) = \\sum_{t = 0}^T R(s_t)$$\n",
    "\n",
    "Where, the expected rewards, $\\mathcal{R}(s')$, for the transition from state $s$, to the next state, $s'$, given the transition reward matrix, $\\mathcal{R}_{s s'}$, and the transition probability matrix $\\mathcal{P}_{ss'}$:   \n",
    "\n",
    "$$\\mathcal{R}(s') = E \\big[ \\mathcal{R}_{s s'}\\ |\\ S_t = s \\big]$$  \n",
    "$$Reward(s') = Probability\\ of\\ transition\\ from\\ s\\ to\\ s' * Rewards_{ss'}$$\n",
    "\n",
    "\n",
    "But, consider what happens with an infinite Markov reward process, which never reaches a terminal state. If we use the above formulation, the utility will grow without bound; e.g. $U(s_t) \\rightarrow \\infty$ as $T \\rightarrow \\infty$. \n",
    "\n",
    "The solution to keeping gain bounded for infinite Markov reward processes is **discounting**. By discounting we are saying that the value of a reward in the future decreases the further in the future the reward is received. This is a commonly used concept in many fields. For example, an investor will discount expected future returns, preferring immediate payoff. \n",
    "\n",
    "Using discounting, we can formulate a bounded relationship between gain and reward:\n",
    "\n",
    "$$G([s_o, s_1, s_2, s_3 \\ldots]) = R(s_o) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\gamma^3 R(s_3) \\ldots = \\sum_{t = 0}^{\\infty} \\gamma^{t} R(s_t)$$\n",
    "\n",
    "The choice of the discount parameter, $\\gamma$, will change the outcome for the Markov reward process:\n",
    "- As $\\gamma \\rightarrow 0$, the reward process becomes myopic, only counting near term rewards.\n",
    "- As $\\gamma \\rightarrow 1$, the reward process becomes far sighted, valuing distant rewards highly. \n",
    "\n",
    "For infinite Markov reward processes we are interested in the **return** for state transitions starting with the current state. Return is the sum of the rewards for future state transitions and can be expressed as:\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\ldots = \\sum_{k = 0}^{\\infty} \\gamma^{k} R_{t+k+1}$$\n",
    "\n",
    "We are also interested in the **state value function**. This expression computes the expected future value of being in state $s$:\n",
    "\n",
    "$$v(s) = E[G_t\\ |\\ S_t = s ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policy\n",
    "\n",
    "The agent uses a **policy**, $\\pi$, to determine which action to take. The expected **action value** given the action, $a$, from state, $s$, by the policy, $\\pi$, is:   \n",
    "\n",
    "$$q_{\\pi}(s,a) = \\mathbb{E}_{\\pi} [G_{t}\\ |\\ S_t = s, A_t = a] $$\n",
    "\n",
    "Our goal is to find an **optimal policy**, $\\pi^*$, which maximizes the expected action value. We say that the optimal policy gives the highest expected action value, , $q_*(s,a)$,, for the action $a$, from state $s$:\n",
    "\n",
    "$$q_{\\pi^*}(s,a) = \\mathbb{E}_{\\pi^*} [G_{t}\\ |\\ S_t = s, A_t = a] $$\n",
    "\n",
    "An optimal policy has an expected action value greater than or equal to all possible policies:\n",
    "\n",
    "$$q_{\\pi^*}(s,a) \\ge q_{\\pi}(s,a)\\ \\forall\\ \\pi$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Example\n",
    "\n",
    "> **Exercise 8-1-2:** With the above theory in mind, you will now try a computational example for a Markov reward process. The convergence properties of the state value function are of particular interest. This convergence is key to finding an optimal policy for a Markov reward process.     \n",
    "> As a first step you, must define a matrix of the rewards for transitions between the states. Fill in the missing code below using the transition rewards shown in the diagram above. Then execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in the array belo to define the Markov reward matrix\n",
    "R = \n",
    "\n",
    "\n",
    "print('The reward matrix for state transitions')\n",
    "labels = ['OldCar','OldBreak','NewCar','NewBreak','Accident']\n",
    "print(pd.DataFrame(R, columns = labels, index = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the cell below, complete the code to perform the following operations:      \n",
    "> 1. Compute the reward, as the dot product between the reward matrix and the difference between the current state and the last state.    \n",
    "> 2. Save the current stat to the `last_state` variable.  \n",
    "> 3. Apply the state transition matrix to update the current state.  \n",
    "> Execute your completed code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_gain(T, R, s, n = 1, gamma = 0.9):\n",
    "    gain_list = np.reshape(np.zeros(s.shape), (1,s.shape[0]))\n",
    "    state = s\n",
    "    last_state = s\n",
    "    for _ in range(n):\n",
    "        ## Complete the code below\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Add the updated reward to the discounted sum of rewards to upate the gain \n",
    "        reward = reward + gamma * gain_list[-1,:]\n",
    "        gain_list = np.concatenate((gain_list, np.reshape(reward, (1,s.shape[0]))), axis = 0)        \n",
    "    return np.delete(gain_list, 0, 0) \n",
    "    \n",
    "gains = compute_state_value(T, R, initial_state, n = 100)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, execute the code in the cell below to display the gain function values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gain(gain, labels):\n",
    "    fig = plt.figure(figsize=(6,6)) # define plot area\n",
    "    ax = fig.gca() # define axis \n",
    "    ax.set_xlabel('Number of time steps')\n",
    "    ax.set_ylabel('Gain')\n",
    "    ax.set_title('Gain vs. number of time steps')\n",
    "    for i in range(gain.shape[1]):\n",
    "        plt.plot(gain[:,i])\n",
    "    ax.legend(labels)    \n",
    "    \n",
    "plot_gain(gains, labels)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 1.** Have these gain functions reached a steady and state and why?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Execute the code in the cell below to compute and display the gain function for a discounting constant, $\\gamma=0.99$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = compute_gain(T, R, initial_state, n = 40000, gamma=0.99)  \n",
    "plot_rewards(gain, labels)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question 2:** Compare the gain curves for $\\gamma=0.9$ and $\\gamma=0.99$. Describe the difference in convergence to steady state for both discount constants.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, 2019, 2022, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
