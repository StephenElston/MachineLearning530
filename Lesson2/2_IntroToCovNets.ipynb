{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning \n",
    "## Introduction to Convolutional Neural Networks\n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Introduction to convolutional neural networks\n",
    "\n",
    "This lesson introduces you to a powerful neural network architecture, known as **convolutional neural networks**. Convolutional neural networks operate by **learning a set of filters**  or **convolution kernels**. Using a process, known as **convolution**, these filters extract a **feature map** from the raw data. The feature map is a **lower dimensional** map of the raw input features. This map is learned in a supervised machine learning process employing back propagation. You can think of convolutional neural networks as a powerful and flexible **dimensionality reduction** method. \n",
    "\n",
    "Convolutional neural networks are used to build feature maps from any type of data with coherency in one or more dimensions. This includes time series data, and text data as one dimensional cases, and image data as a two dimensional case. \n",
    "\n",
    "Convolutional neural networks have a resonably long history. The first known commerical application was for automated processing of check images by LeCun et. al. (1998). For unclear reasons, convolutional neural networks were religated to specialized applications such as hand writing recognition until Krizhevsky et. al. (2012) used them to deciscively win an ImageNet object recognition competition. Curiously, other teams had previously used convolutional neural networks to win competitons, but somehow this was not widely recognized. \n",
    "\n",
    "In this lesson you will learn:\n",
    "1. The basics of convolutional neural network architecture.\n",
    "2. How convolutional neural networks learn the filters to create the feature map. \n",
    "3. Feature extraction for creating the feature map. \n",
    "4. Visualization of the feature map and the filters.\n",
    "5. Commonly used convolutional opertor design. \n",
    "6. How to use the feature map in suppervised learning. \n",
    "\n",
    "****\n",
    "**Note:** The formulation of convolution filters used in deep learning is somewhat idiosycratic. If you are familar with the standard approach as applied in say electrical engineering or control engineering you may find the approach used here a bit different. \n",
    "****\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.datasets import mnist\n",
    "import keras.utils.np_utils as ku\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Applying 2-d convolution\n",
    "\n",
    "Let's review how we can apply convolution operations in two dimensions. Figure 1.1 illustrates a simple example of applying a convolution operator to a 2-d input array. The 2-d input array can be a gray-scale image, for example. \n",
    "\n",
    "A 3X3 convolutional operator is illustrated in Figure 1.1. This operator is applied to a small image of dimension of 4X4 pixels. Within the 4X4 image the operator is only applied four times, where the complete operator lies on the image. Convolution where the operator is completely contained within the input sample is sometimes called call **valid convolution**. \n",
    "\n",
    "In this case the 4X4 pixel input results in a 2X2 pixel output. You can think of the convolutional operator mapping from the center of the operator input to pixel in the output. Similar to the 1-d case, for a convolution operator with span $s$ in both dimensions the output array will be reduced in each dimension by $\\frac{s + 1}{2}$\n",
    "\n",
    "![](img/2D-Conv.JPG)\n",
    "<center>Figure 1.1. Simple 2-d convolution operation</center>\n",
    "\n",
    "For a convolution with the identical span $s$ in both dimensions we can write the convolution operations as: \n",
    "\n",
    "$$S(i,j) = (I * K)(i,j) = \\sum_{m = {i - a}}^{i + a} \\sum_{n = j - a}^{j + a} I(i, j) K(i-m, j-n)$$\n",
    "\n",
    "Notice that $S$, $I$ and $K$ are all tensors. Given this fact, the above formula is easily extended to higher dimensional problems. \n",
    "\n",
    "The convoluational kernel $K$ and the image $I$ are commutative so there is an alternative representation by applying an operation known as **kernel flipping**. Flipped kernel convolution can be represented as follows:\n",
    "\n",
    "$$s(i,j) = (I * K)(i,j) = \\sum_{m = {i - a}}^{i + a} \\sum_{n = j - a}^{j + a} I(i-m, j-n) K(i, j)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convolution in higher dimensions\n",
    "\n",
    "In deep convolutional neural networks the input typically have several **input channels** with the convolution operation resulting in multiple **output channels**. \n",
    "\n",
    "So far, we have only looked at single channel convolution. Here are some examples of input tensors with different numbers of input channels:\n",
    "- A gray-scale image has a single input channel of pixels. \n",
    "- A color image typically has 3 input channels, {Red, Green, Blue}.\n",
    "- A color movie has a large number of input channels. For $n$ images in the video there are $3 * n$ images counting the Red, Green, Blue channels for each image.  \n",
    "\n",
    "The purpose of the a convolutional neural network is to create a feature map. To create the most general feature map convolution operators need not be restricted to a single input channel. \n",
    "\n",
    "Further, the feature map can can many channels, resulting in multiple output channels. Each channel is a set of features extracted from the input tensor. This concept is illustrated in Figure 1.2 below. In this figure a set of convolution operators transform the input tensors to multiple channels of the feature map. Each convolution operator maps to a specific output channel in the feature map. \n",
    "\n",
    "![](img/MultiChannel.JPG)\n",
    "<center>Figure 1.2. Example of multichannel convolution</center>\n",
    "\n",
    "Let's investigate the case of a 3-d input tensor $V$, a 3-d output tensor $Z$ and 4-d convolution tensor $K$. In this case the convolution equation becomes:\n",
    "\n",
    "$$Z_{i,j,k} = (V * Z)(i,j,k,l) = \\sum_{l} \\sum_{m = -a}^{a}\n",
    "\\sum_{n = -a}^{a}\n",
    "V_{i,j,l} \\cdot K_{i-m,j-n,k,l}$$\n",
    "where,  \n",
    "$i,j$ are the spatial dimensions,  \n",
    "$k$ is the index of the output channel,  \n",
    "$l$ is the index of the input channel,  \n",
    "$K_{i,j,k,l}$ is the kernel connecting the $l$th channel of the input to the $k$th channel of the output for pixel offsets $i$ and $j$    \n",
    "$V_{i,j,l}$ is the $i,j$ input pixel offsets from channel $l$ of the input,  \n",
    "$Z_{i,j,k}$ is the $ i,j$ pixel offsets from channel $k$ of the output,  \n",
    "$a = \\frac{1}{2}(kernel\\_span + 1)$, for an odd length kernel.  \n",
    "\n",
    "Notice that the summation in the equation above is over input channels $l$ as well as the spatial dimensions $m,n$ of the convolutional operator. This operation is applied for each spatial pixel coordinate $i,j$. This generalization allows the convolutional kernel to operate over multiple input channels. This is property is highly desirable since features should be extracted from the entire input tensor. \n",
    "\n",
    "The entire forgoing operation is performed for each output channel $k$. Therefore, you can think of the $k$th dimension of the 4-d kernel tensor as indexing multiple 3-d convolution kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parameter sharing and sparse interactions\n",
    "\n",
    "**Parameter sharing** and **sparse interactions** are two powerful aspects of convolutional neural networks. To understand what these terms mean and why they are important, consider the fully connected neural network in Figure 1.3 below.  \n",
    "![](img/FullyConnected.JPG)\n",
    "<center>Figure 1.3. A fully connected network</center>\n",
    "\n",
    "The fully connected neural network has a large number of weights, $5^2 = 25$ in this case. Compare this network to the convolutional layer with $2 \\times 2$ operator will have only $2*2 = 4$ weights. For a more realistic case, the fully connected network might have millions of weights. The convolutional network using **weight sharing** across the entire layer. For a $5 \\times 5$ CovNet layer there are only $5 * 5 = 25$ weights. Parameter sharing is also referred to as **tied weights**. This is an example of a **sparse interaction**.\n",
    "\n",
    "The shared nature of the parameters in convolutional neural networks is a significant reduction in complexity. Statistically, such a sparse model is said to **share statistical strength**. This statistical strength results in model weights with less uncertainty and a model less likely to be over-fit.  \n",
    "\n",
    "The computational efficiency of convolutional neural networks is not just in training. Convolutional neural networks are also efficient for creating **rich feature maps** which lead to high levels of accuracy in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Pooling and invariance\n",
    "\n",
    "To further reduce dimensionality and to make the feature map robust to small shifts in the input, **pooling** is applied following the nonlinear detection stage. Pooling involves applying an aggregation function to patches of the output from the nonlinear detector of a convolution layer. \n",
    "\n",
    "Researchers have tried a number of pooling schemes. In practice **max pooling** (Zhou and Chellappa, 1988) has proved to be both robust and versatile. In max pooling the output value is just the maximum of the input values in each patch. \n",
    "\n",
    "Unlike convolutional layers, there are no parameters to learn in pooling layers. \n",
    "\n",
    "Max pooling has the nice property of being robust to small shifts in the input values. For example, Figure 1.4 shows the response of max pooling to a right sift of a single pixel for an operator with a span of 3. Notice that the response itself is simply shifted one pixel to the right as well. The pattern of maximum values in the feature map (the output) remains the same. \n",
    "\n",
    "![](img/ShiftOne.JPG)\n",
    "<center>Figure 1.4.   \n",
    "    Example of max pooling responses to one pixel shift right</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2-1:** You will now undertake an exercise to better understand the effects of max pooling. To simplify this exercise you will work with 1-dimensional data and **edge detection** operators. Do the following:   \n",
    "> 1. Create a function named `max_pool` that moves a maximum of the absolute value operator with a span of 3 and stride of 2 over the input series. For simplicity start the operator at the beginning of the series.     \n",
    "> 2. Create two edge detection convolutional kernels, $kernel1 = [-0.5, 1.0, -0.5]$ and $kernel2 = [0.5, -1.0, 0.5]$.   \n",
    "> 3. Apply both of the convolution kernels to the `series1` provided using the [numpy.convolve](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html) function. Assign these two results to separate variables.    \n",
    "> 4. Apply your `max_pool` function to the convolution result series. Assign these two results to different variables.    \n",
    "> 5. Display the results comparing the convolution series to the max-pooled series using the `plot_pool` function provided.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_pool(series1, series2, pool1, pool2):\n",
    "    x = list(range(series1.shape[0]))\n",
    "    end = series1.shape[0]\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ## First channel\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, series1, label = 'Detected series')\n",
    "    plt.plot(x[1:end:2], pool1, color = 'red', label = 'First channel max pool')\n",
    "    plt.title('Series of raw values and max pool values')\n",
    "    ## Second channel\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, series2, label = 'Detected series')\n",
    "    plt.plot(x[1:end:2], pool2, color = 'red', label = 'Second channel Max pool')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Value') \n",
    "    \n",
    "nr.seed(12233)\n",
    "series1 = np.concatenate((np.zeros((20,)), np.zeros((5,)) + 1.0, np.zeros((5,)) - 1.0, np.zeros((20,))))\n",
    "# series2 = np.concatenate((np.array([0.0]), out.append(np.max(np.abs(series[i:i+span+1])))series1))\n",
    "\n",
    "## Your code goes below\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine these results and answer these questions:   \n",
    "> 1. How does the max-pooling operator preserve edges in the convolution output series?     \n",
    "> 2. Haw can you explain the invariance of the max-pooling result to the sign of the input series.  \n",
    "> **End of exercise**.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.     \n",
    "> 2.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Stride and tiling\n",
    "\n",
    "Up until now, we have only moved the convolution operator one pixel at a time. There is no restriction that requires this type of step, however. In fact, a convolutional operator or pooling opertor can be moved by several pixels in any direction. This step size is known as the **stride** of the operator. \n",
    "\n",
    "Operators with stride greater than one **decimate** the number of pixels in the output. This can be useful when resizing images of different dimensions so they have constant input dimensions for a neural network. \n",
    "\n",
    "Using an operator with $stride = span$ is a special case known as **tiling**. Tiled operators are layed out without any overlap. Figure 1.6 shows an example of tiling. In this case, an input of dimensions $6X6$ is tiled by operators with dimension $3X3$ with stride of 3. Only 4 tiles fit on the input in this case, resulting in a $4X4$ output array.\n",
    "\n",
    "![](img/Tiled.JPG)\n",
    "<center>**Figure 1.6. Example of a tiled convolution operator**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Convolutional neural networks with Keras\n",
    "\n",
    "With the foregoing theory and simple examples, let's try constructing and testing a convolutional neural network using Keras. In this case, we will test classification of of the MNIST dataset. The neural network will have the following layers:\n",
    "1. An input layer for the 28X28 images.\n",
    "2. A multi-layer convolutional neural network to create a feature map.\n",
    "3. A fully-connected hidden layer to perform the classification.\n",
    "4. A output layer to indicate which digit is most likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing the dataset\n",
    "\n",
    "The preparation of this dataset is follows nearly identical steps to the process followed in an earlier lesson. In summary, three preparation steps are performed:\n",
    "1. Load the training and test image data and labels.\n",
    "2. Reshape the image tensors so the neural network views them as a 4-d tensor. For the training images the tensor has a shape of (60000,28,28,1) corresponding to 60000 images of dimension $28 \\times 28$ with a single channel (these are gray-scale images). At the same time the pixel values are converted to `float` type in the range $\\{ 0,1 \\}$.\n",
    "3. The labels are recoded as dummy variables using the Keras [to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float')/255\n",
    "print(train_images.shape)\n",
    "print(train_images.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_images.shape, test_labels.shape)\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float')/255\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = ku.to_categorical(train_labels)\n",
    "print(train_labels[5:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = ku.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Defining the model. \n",
    "\n",
    "It is now time to define the model. \n",
    "\n",
    "As a first step, the callbacks required to implement early stopping regularization are defined. The steps are:    \n",
    "1. Define a path to an HDF5 file used to save the training history. \n",
    "2. The first callback stops the training when the validation loss does not improve within 4 epochs, the patience.  \n",
    "3. The second callback saves the model weights resulting from the epoch with the best validation loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up and call-backs for early stopping\n",
    "filepath = 'my_model_file.hdf5' # define where the model is saved\n",
    "callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss', # Use accuracy to monitor the model\n",
    "            patience = 4 # Stop after one step with lower accuracy\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = filepath, # file where the checkpoint is saved\n",
    "            monitor = 'val_loss', # Don't overwrite the saved model unless val_loss is worse\n",
    "            save_best_only = True # Only save model if it is the best\n",
    "        )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2-2:** You will now use Keras to define the CovNet model named `nn`. This model uses both convolutional layers and max pooling layers. You can find [documentation on the convolutional layers in the Keras documentation](https://keras.io/layers/convolutional/). You can find [documentaton on pooling layers in the Keras documentation](https://keras.io/layers/pooling/).\n",
    "> The architecture of the model you will create is:\n",
    "> 1. A 3X3 convolutional layer with 32 output channels using the [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2D/) function, with `activation='relu'`. Recall that this operation creates a feature map with 32 channels from the single gray-scale input channel. Make sure you define the input shape. \n",
    "> 2. A 2X2 max pooling layer using the [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) function.\n",
    "> 3. A 3X3 convolution layer with 64 output channels, with `activation='relu'`. The 32 input channels are mapped to a 64 channel output feature map. \n",
    "> 4. Another 2X2 max pooling layer. \n",
    "> 5. A final 3X3 convolution layer with 64 output channels, with `activation='relu'`. \n",
    "> 6. The 64 channel feature map is flattened to a vector.\n",
    "> 7. A Dense (fully-connected) hidden layer with 64 units is the first classifier layer, with `activation='relu'`. A limited amount of regularization is applied using Keras [layer regularizer l2](https://keras.io/api/layers/regularizers/) as a ` kernel_regularizer`, with hyperparameter 0.1. This layer operates on the flattened feature map. \n",
    "> 8. A dropout layer is used to improve regularization of the model using the [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) function with argument 0.5. \n",
    "> 9. The output Dense layer has 10 units with `activation='softmax'` to indicate the classification of the digits.\n",
    "> 10. A summary of the model is printed. \n",
    "> Execute your code and examine the summary of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Your code goes below\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is quite bit of interesting information in the summary of this model:\n",
    "> 1. How does the dimensionality of the feature map change at each convolution and max pooling layer? \n",
    "> 2. Compare the number of parameters of your CovNet model to the to the fully connected model you created for exercise 1-4 of Assignment 1. How different is the overall number of parameters and layers? \n",
    "> 3. What does the difference in the numbers of parameters between the CovNet and the fully connected model tell you about the ability to train these models with a finite amount of data? \n",
    "> 4. How many learnable parameters do the max-pooling layers have? \n",
    "> **End of exercise**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.   \n",
    "> 2.     \n",
    "> 3.     \n",
    "> 4.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train and test the model\n",
    "\n",
    "Now, it is time to compile and fit the model. Run the code in the cell below, which may take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "nn.compile(optimizer = 'RMSprop', loss = 'categorical_crossentropy', \n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "## Now fit the model\n",
    "history = nn.fit(train_images, train_labels, \n",
    "                  epochs = 40, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code in the cells below to plot the loss history vs. epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    '''Function to plot the loss vs. epoch'''\n",
    "    train_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "    x = list(range(1, len(test_loss) + 1))\n",
    "    plt.plot(x, test_loss, color = 'red', label = 'Test loss')\n",
    "    plt.plot(x, train_loss, label = 'Training losss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Epoch')\n",
    "    \n",
    "plot_loss(history)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    train_acc = history.history['accuracy']\n",
    "    test_acc = history.history['val_accuracy']\n",
    "    x = list(range(1, len(test_acc) + 1))\n",
    "    plt.plot(x, test_acc, color = 'red', label = 'Test accuracy')\n",
    "    plt.plot(x, train_acc, label = 'Training accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs. Epoch')  \n",
    "    \n",
    "plot_accuracy(history)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2-3:** Examine these results and answer the following questions:    \n",
    "> 1. How does the accuracy and error rate of your CovNet model compare to the regularized fully connected model of Exercise 1-7 in Assignment 1?\n",
    "> 2. How can you characterize the convergence or learning rate of the CovNet model after the first 10 epochs.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.        \n",
    "> 2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 What features does the convolutional neural network learn?\n",
    "\n",
    "You might well ask, what is in the various layers of the feature map? It turns out, we can actually visualize the output from each layer of the convolutional neural network to get a feel for the learned features. This can be done for both convolutional and max pooling layers.\n",
    "\n",
    "As a first step we must pick and image to use as the test case. For this example, we arbitrarily choose the 13th image in the training tensor. Execute the code in the cell below to visualize this image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_images[12,:,:,:]\n",
    "print(img.shape)\n",
    "plt.imshow(img.reshape((28,28)), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sample image turns out to be a number 3. \n",
    "\n",
    "The code in cell below does the following:\n",
    "1. Extracts the layers from the model object into a list. \n",
    "2. Creates an activation model for the set of layers using the input of the original model object and the list of layer outputs. \n",
    "3. Computes the activations for each layer by applying the predict method to the activation model. \n",
    "\n",
    "Execute this code to create a list of activations for the convolutional neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in nn.layers[:7]]\n",
    "activation_model = models.Model(inputs = nn.input, outputs = layer_outputs)\n",
    "activations = activation_model.predict(img.reshape(1,28,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below does the following:\n",
    "1. Iterates through the activations of the first 5 model layers.\n",
    "2. Iterates over the activation channel in the layer displaying the image. There is one image for each channel in the feature map. \n",
    "\n",
    "Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j in range(5):\n",
    "    fig_shape = activations[j].shape\n",
    "    s = fig_shape[3]/32\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    for i in range(fig_shape[3]):\n",
    "        ax = fig.add_subplot(int(s*4),8,(i+1))\n",
    "        plt.imshow(activations[j].reshape((fig_shape[1],fig_shape[2],fig_shape[3]))[:,:,i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2-4:** Examine the above sets of images. There are 5 sets of images, one for each of the output of the first 5 layers of the model. For each layer, there is an image showing one channel of the feature map.  \n",
    "> 1. The initial single channel $28 \\times 28 \\times 1$ input image tensor becomes a $32 \\times 26 \\times 26 \\times 1$, 32 channel feature map after the first convolution. The second $32 \\times 13 \\times 13 \\times 1$ output is from the max-pooling layer. In general terms, describe how much this feature map resembles the original image.   \n",
    "> 2. Examine the feature maps created by the third (convolution), forth (max-pooling), and fifth (convolution) layers. How can you best describe the level of abstraction of these layers and the the resemblance to the original image?      \n",
    "> 3. What effect can you observe of using the max-pooling layer other than dimensionality reduction?  \n",
    "> 4. WHat relationship can you identify between the dimensionality of the feature map and the level of abstraction of the features? \n",
    "> **End of exercise**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.      \n",
    "> 2.       \n",
    "> 3.      \n",
    "> 4.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Padding convolution\n",
    "\n",
    "As you have observed, each convolution layer in a neural network reduces the spatial dimensions by $\\frac{span+1}{2}$ for odd span kernels. The coverage of the convolution opertor can be expanded by **zero padding** in the spatial dimenstion. With the zero padding added, the convolution operator covers the entire spatial dimension of the input tensor. This type of convolution is sometime referred to as **same convolution**, since the output tensor has the same spatial dimensions. \n",
    "\n",
    "In some situations, adding zero padding produces a better feature map. Zero padding allows each pixel to be visited the same number of times by the convolution operator. Without padding pixels at the edge, are not full represented in the feature map. More importantly, the larger feature map will have bigger model capacity. \n",
    "\n",
    "2-d convolution with zero padding is illustrated in Figure 4.1 below. The original input 2-d input tensor is shown in gray. The span of the kernel is 3X3 so one additional column of zeros is added to each sided along with a row of zeros top and bottom. As illustrated, the kernel can operate on the edge pixels. The result is an output tensor with the same spatial dimension as the input. \n",
    "\n",
    "![](img/Padding.JPG)\n",
    "<center>**Figure 4.1. 2-d convolution with zero padding**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2-5:** With Keras, adding zero padding to convolutional layeres is easy. The `padding = same` argument adds zero padding. The default is `padding = valid` which adds no padding. Create a new model, with name 'nns', with the same layers as the first model but with the `padding = same` argument for the convolutional layers.  Then print the summary of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Your code goes below\n",
    "nns = models.Sequential()\n",
    "\n",
    "## Add some convolutional layers to extract features \n",
    "## from the images with padding added to the convolution layers.\n",
    "nns.add(layers.Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (28, 28, 1)))\n",
    "nns.add(layers.MaxPooling2D((2, 2)))\n",
    "nns.add(layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "nns.add(layers.MaxPooling2D((2, 2)))\n",
    "nns.add(layers.Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "\n",
    "## Now flatten the output of the convolutional layers so \n",
    "## a fully connected network can be applied.\n",
    "nns.add(layers.Flatten())\n",
    "\n",
    "## Finally, fully connected layers to classify the digits \n",
    "## Using the features extracted by the convolutional layers.\n",
    "nns.add(layers.Dense(64, activation = 'relu',\n",
    "                        kernel_regularizer=regularizers.l2(0.1)))\n",
    "nns.add(Dropout(0.5))\n",
    "nns.add(layers.Dense(10, activation = 'softmax'))\n",
    "\n",
    "nns.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the summary of your model and answer these questions:   \n",
    "> 1. The final feature map is $64 \\times 7 \\times 7$, rather than $64 \\times 3 \\times 3$ for the model without padding. This feature map has 49 pixels per channel, compared to 9 pixels per channel previously. Do you expect that this feature map will contain more information and why? \n",
    "> 2. How has the number of parameters changed for the convolutional layer, and is this expected? \n",
    "> 3. Compare the number of learnable parameters for the 64 unit fully connected layer between the models with and without padding. How can you account for this difference? \n",
    "> **End of exercise**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.     \n",
    "> 2.     \n",
    "> 3.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next execute the code in the cell below to compile and fit the model. This process will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "nns.compile(optimizer = 'RMSprop', loss = 'categorical_crossentropy', \n",
    "                metrics = ['accuracy'])\n",
    "    \n",
    "## Now fit the model\n",
    "history_s = nns.fit(train_images, train_labels, \n",
    "                  epochs = 40, batch_size = 256,\n",
    "                  validation_data = (test_images, test_labels),\n",
    "                  callbacks = callbacks_list,  # Call backs argument here\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cells below to plot the loss vs. epoch and accuracy vs. epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history_s)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history_s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results do not appear significantly difference than for the unpadded model. It may well be the case that richer feature map may not be important for this relatively simple problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, 2019, 2020, 2021, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
